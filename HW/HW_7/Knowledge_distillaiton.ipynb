{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import torch\r\n",
    "import os\r\n",
    "import torch.nn as nn\r\n",
    "import torch.optim as optim\r\n",
    "import torch.nn.functional as F\r\n",
    "import torchvision.models as models\r\n",
    "import re\r\n",
    "import torch\r\n",
    "from glob import glob\r\n",
    "from PIL import Image\r\n",
    "import torchvision.transforms as transforms"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\r\n",
    "\r\n",
    "    def __init__(self, folderName, transform=None):\r\n",
    "        self.transform = transform\r\n",
    "        self.data = []\r\n",
    "        self.label = []\r\n",
    "\r\n",
    "        for img_path in sorted(glob(folderName + '/*.jpg')):\r\n",
    "            try:\r\n",
    "                # Get classIdx by parsing image path\r\n",
    "                class_idx = int(re.findall(re.compile(r'\\d+'), img_path)[1])\r\n",
    "            except:\r\n",
    "                # if inference mode (there's no answer), class_idx default 0\r\n",
    "                class_idx = 0\r\n",
    "\r\n",
    "            image = Image.open(img_path)\r\n",
    "            # Get File Descriptor\r\n",
    "            image_fp = image.fp\r\n",
    "            image.load()\r\n",
    "            # Close File Descriptor (or it'll reach OPEN_MAX)\r\n",
    "            image_fp.close()\r\n",
    "\r\n",
    "            self.data.append(image)\r\n",
    "            self.label.append(class_idx)\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.data)\r\n",
    "\r\n",
    "    def __getitem__(self, idx):\r\n",
    "        if torch.is_tensor(idx):\r\n",
    "            idx = idx.tolist()\r\n",
    "        image = self.data[idx]\r\n",
    "        if self.transform:\r\n",
    "            image = self.transform(image)\r\n",
    "        return image, self.label[idx]\r\n",
    "\r\n",
    "\r\n",
    "trainTransform = transforms.Compose([\r\n",
    "    transforms.RandomCrop(256, pad_if_needed=True, padding_mode='symmetric'),\r\n",
    "    transforms.RandomHorizontalFlip(),\r\n",
    "    transforms.RandomRotation(15),\r\n",
    "    transforms.ToTensor(),\r\n",
    "])\r\n",
    "testTransform = transforms.Compose([\r\n",
    "    transforms.CenterCrop(256),\r\n",
    "    transforms.ToTensor(),\r\n",
    "])\r\n",
    "\r\n",
    "def get_dataloader(mode='training', batch_size=32):\r\n",
    "\r\n",
    "    assert mode in ['training', 'testing', 'validation']\r\n",
    "\r\n",
    "    dataset = MyDataset(\r\n",
    "        f'../HW_3/data/{mode}',\r\n",
    "        transform=trainTransform if mode == 'training' else testTransform)\r\n",
    "\r\n",
    "    dataloader = torch.utils.data.DataLoader(\r\n",
    "        dataset,\r\n",
    "        batch_size=batch_size,\r\n",
    "        shuffle=(mode == 'training'))\r\n",
    "\r\n",
    "    return dataloader\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "train_dataloader = get_dataloader('training', batch_size=16)\r\n",
    "valid_dataloader = get_dataloader('validation', batch_size=16)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "'''\r\n",
    "簡單上來說就是讓已經做得很好的大model們去告訴小model\"如何\"學習。 而我們如何做到這件事情呢? 就是利用大model預測的logits給小model當作標準就可以了。\r\n",
    "\r\n",
    "為甚麼這會work?\r\n",
    "1 例如當data不是很乾淨的時候，對一般的model來說他是個noise，只會干擾學習。透過去學習其他大model預測的logits會比較好。\r\n",
    "2 label和label之間可能有關連，這可以引導小model去學習。例如數字8可能就和6,9,0有關係。\r\n",
    "3 弱化已經學習不錯的target(?)，避免讓其gradient干擾其他還沒學好的task。\r\n",
    "\r\n",
    "'''\r\n",
    "#TODO 数据集读取 和训练  预测 保存"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\n簡單上來說就是讓已經做得很好的大model們去告訴小model\"如何\"學習。 而我們如何做到這件事情呢? 就是利用大model預測的logits給小model當作標準就可以了。\\n\\n為甚麼這會work?\\n1 例如當data不是很乾淨的時候，對一般的model來說他是個noise，只會干擾學習。透過去學習其他大model預測的logits會比較好。\\n2 label和label之間可能有關連，這可以引導小model去學習。例如數字8可能就和6,9,0有關係。\\n3 弱化已經學習不錯的target(?)，避免讓其gradient干擾其他還沒學好的task。\\n\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def loss_fn_kd(output,labels,teacher_output,T=20,alpha=0.5):\r\n",
    "    hard_loss = F.cross_entropy(output,labels)*(1. -alpha)\r\n",
    "    soft_loss = nn.KLDivLoss(reduction ='batchmean')(F.log_softmax(output/T,dim=1),\r\n",
    "                                            F.softmax(teacher_output/T,dim=1))* (alpha * T * T)  \r\n",
    "    return hard_loss +soft_loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "#data loader 和hw 3 一样\r\n",
    "class StudentNet(nn.Module):\r\n",
    "    '''\r\n",
    "      在這個Net裡面，我們會使用Depthwise & Pointwise Convolution Layer來疊model。\r\n",
    "      你會發現，將原本的Convolution Layer換成Dw & Pw後，Accuracy通常不會降很多。\r\n",
    "\r\n",
    "      另外，取名為StudentNet是因為這個Model等會要做Knowledge Distillation。\r\n",
    "    '''\r\n",
    "    def __init__(self,base = 16,width_mult = 1):\r\n",
    "        super(StudentNet,self).__init__()\r\n",
    "        multiplier = [1,2,4,8,16,16,16,16]\r\n",
    "        bandwidth =[base*m for m in multiplier]\r\n",
    "\r\n",
    "        for i in range(3,7):\r\n",
    "            bandwidth[i] = int(bandwidth[i]* width_mult)\r\n",
    "        \r\n",
    "        self.cnn = nn.Sequential(\r\n",
    "            nn.Sequential(\r\n",
    "            nn.Conv2d(3,bandwidth[0],3,1,1),\r\n",
    "            nn.BatchNorm2d(bandwidth[0]),\r\n",
    "            nn.ReLU6(),\r\n",
    "            nn.MaxPool2d(2,2,0),\r\n",
    "            ),\r\n",
    "            nn.Sequential(\r\n",
    "                nn.Conv2d(bandwidth[0],bandwidth[0],3,1,1),\r\n",
    "                nn.BatchNorm2d(bandwidth[0]),\r\n",
    "                nn.ReLU6(),\r\n",
    "                nn.Conv2d(bandwidth[0],bandwidth[1],1),\r\n",
    "                nn.MaxPool2d(2,2,0),\r\n",
    "            ),\r\n",
    "            nn.Sequential(\r\n",
    "                nn.Conv2d(bandwidth[1], bandwidth[1], 3, 1, 1, groups=bandwidth[1]),\r\n",
    "                nn.BatchNorm2d(bandwidth[1]),\r\n",
    "                nn.ReLU6(),\r\n",
    "                nn.Conv2d(bandwidth[1], bandwidth[2], 1),\r\n",
    "                nn.MaxPool2d(2, 2, 0),\r\n",
    "            ),\r\n",
    "            nn.Sequential(\r\n",
    "                nn.Conv2d(bandwidth[2], bandwidth[2], 3, 1, 1, groups=bandwidth[2]),\r\n",
    "                nn.BatchNorm2d(bandwidth[2]),\r\n",
    "                nn.ReLU6(),\r\n",
    "                nn.Conv2d(bandwidth[2], bandwidth[3], 1),\r\n",
    "                nn.MaxPool2d(2, 2, 0),\r\n",
    "            ),\r\n",
    "            #bandwidth 16\r\n",
    "            nn.Sequential(\r\n",
    "                nn.Conv2d(bandwidth[3], bandwidth[3], 3, 1, 1, groups=bandwidth[3]),\r\n",
    "                nn.BatchNorm2d(bandwidth[3]),\r\n",
    "                nn.ReLU6(),\r\n",
    "                nn.Conv2d(bandwidth[3], bandwidth[4], 1),\r\n",
    "            ),\r\n",
    "            nn.Sequential(\r\n",
    "                nn.Conv2d(bandwidth[4], bandwidth[4], 3, 1, 1, groups=bandwidth[4]),\r\n",
    "                nn.BatchNorm2d(bandwidth[4]),\r\n",
    "                nn.ReLU6(),\r\n",
    "                nn.Conv2d(bandwidth[4], bandwidth[5], 1),\r\n",
    "            ),\r\n",
    "            nn.Sequential(\r\n",
    "                nn.Conv2d(bandwidth[5], bandwidth[5], 3, 1, 1, groups=bandwidth[5]),\r\n",
    "                nn.BatchNorm2d(bandwidth[5]),\r\n",
    "                nn.ReLU6(),\r\n",
    "                nn.Conv2d(bandwidth[5], bandwidth[6], 1),\r\n",
    "            ),\r\n",
    "            nn.Sequential(\r\n",
    "                nn.Conv2d(bandwidth[6], bandwidth[6], 3, 1, 1, groups=bandwidth[6]),\r\n",
    "                nn.BatchNorm2d(bandwidth[6]),\r\n",
    "                nn.ReLU6(),\r\n",
    "                nn.Conv2d(bandwidth[6], bandwidth[7], 1),\r\n",
    "            ),\r\n",
    "            # 這邊我們採用Global Average Pooling。\r\n",
    "            # 如果輸入圖片大小不一樣的話，就會因為Global Average Pooling壓成一樣的形狀，這樣子接下來做FC就不會對不起來。\r\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\r\n",
    "        )\r\n",
    "        self.fc = nn.Sequential(\r\n",
    "            nn.Linear(bandwidth[7],11),\r\n",
    "        )\r\n",
    "    def forward(self, x):\r\n",
    "        out = self.cnn(x)\r\n",
    "        out = out.view(out.size()[0], -1)\r\n",
    "        return self.fc(out)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "teacher_net = models.resnet18(pretrained=False, num_classes=11).cuda()\r\n",
    "student_net = StudentNet(base=16).cuda()\r\n",
    "\r\n",
    "teacher_net.load_state_dict(torch.load(f'./teacher_resnet18.bin'))\r\n",
    "optimizer = optim.AdamW(student_net.parameters(), lr=1e-3)\r\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "#training \r\n",
    "def run_epoch(dataloader,update = True,alpha = 0.5):\r\n",
    "    total_num,total_hit,total_loss = 0,0,0\r\n",
    "    for now_step,batch_data in enumerate(dataloader):\r\n",
    "        optimizer.zero_grad()\r\n",
    "        inputs,hard_labels = batch_data\r\n",
    "        inputs = inputs.to(device)\r\n",
    "        hard_labels = torch.LongTensor(hard_labels).to(device)\r\n",
    "        \r\n",
    "        with torch.no_grad():\r\n",
    "            soft_labels = teacher_net(inputs)\r\n",
    "        if update:\r\n",
    "            logits = student_net(inputs)\r\n",
    "            loss = loss_fn_kd(logits,hard_labels,soft_labels,20,alpha)\r\n",
    "            loss.backward()\r\n",
    "            optimizer.step()\r\n",
    "        else:\r\n",
    "            with torch.no_grad():\r\n",
    "                logits = student_net(inputs)\r\n",
    "                loss = loss_fn_kd(logits,hard_labels,soft_labels,20,alpha)\r\n",
    "        total_hit += torch.sum(torch.argmax(logits,dim=1)==hard_labels).item()\r\n",
    "        total_num += len(inputs)\r\n",
    "\r\n",
    "        total_loss +=loss.item() *len(inputs)\r\n",
    "    return total_loss /total_num, total_hit/total_num\r\n",
    "\r\n",
    "teacher_net.eval()\r\n",
    "now_best_acc = 0\r\n",
    "for epoch in range(200):\r\n",
    "    student_net.train()\r\n",
    "    train_loss , train_acc = run_epoch(train_dataloader,update = True)\r\n",
    "    student_net.eval()\r\n",
    "    valid_loss,valid_acc = run_epoch(valid_dataloader,update =False)\r\n",
    "\r\n",
    "    if valid_acc >now_best_acc:\r\n",
    "        now_best_acc = valid_acc \r\n",
    "        torch.save(student_net.state_dict(), 'student_model.bin')\r\n",
    "    print('epoch {:>3d}: train loss : {:6.4f}, acc {:6.4f} valid loss: {:6.4f}, acc {:6.4f}'.format(\r\n",
    "        epoch, train_loss, train_acc, valid_loss, valid_acc))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch   0: train loss : 15.2940, acc 0.3020 valid loss: 15.7432, acc 0.3609\n",
      "epoch   1: train loss : 13.8030, acc 0.3813 valid loss: 14.4804, acc 0.4475\n",
      "epoch   2: train loss : 12.8846, acc 0.4313 valid loss: 12.7580, acc 0.5055\n",
      "epoch   3: train loss : 12.1045, acc 0.4700 valid loss: 11.4829, acc 0.5499\n",
      "epoch   4: train loss : 11.4562, acc 0.5004 valid loss: 11.0343, acc 0.5510\n",
      "epoch   5: train loss : 11.1576, acc 0.5214 valid loss: 10.6701, acc 0.5732\n",
      "epoch   6: train loss : 10.4650, acc 0.5451 valid loss: 12.3416, acc 0.5612\n",
      "epoch   7: train loss : 10.1771, acc 0.5545 valid loss: 10.8382, acc 0.5650\n",
      "epoch   8: train loss : 9.7046, acc 0.5697 valid loss: 9.1572, acc 0.6353\n",
      "epoch   9: train loss : 9.3010, acc 0.5895 valid loss: 10.1154, acc 0.5860\n",
      "epoch  10: train loss : 9.2472, acc 0.5880 valid loss: 10.2205, acc 0.6023\n",
      "epoch  11: train loss : 8.8082, acc 0.6050 valid loss: 8.5861, acc 0.6397\n",
      "epoch  12: train loss : 8.5867, acc 0.6096 valid loss: 9.1077, acc 0.6589\n",
      "epoch  13: train loss : 8.4426, acc 0.6249 valid loss: 8.4246, acc 0.6577\n",
      "epoch  14: train loss : 8.3094, acc 0.6303 valid loss: 8.1373, acc 0.6968\n",
      "epoch  15: train loss : 8.2925, acc 0.6328 valid loss: 8.1509, acc 0.6653\n",
      "epoch  16: train loss : 8.0789, acc 0.6428 valid loss: 8.5097, acc 0.6630\n",
      "epoch  17: train loss : 7.8986, acc 0.6543 valid loss: 7.4366, acc 0.6845\n",
      "epoch  18: train loss : 7.7532, acc 0.6511 valid loss: 8.3813, acc 0.6840\n",
      "epoch  19: train loss : 7.6637, acc 0.6617 valid loss: 6.7962, acc 0.7044\n",
      "epoch  20: train loss : 7.5069, acc 0.6634 valid loss: 7.3044, acc 0.7000\n",
      "epoch  21: train loss : 7.5157, acc 0.6700 valid loss: 7.0539, acc 0.7213\n",
      "epoch  22: train loss : 7.2697, acc 0.6770 valid loss: 7.1086, acc 0.7044\n",
      "epoch  23: train loss : 7.2596, acc 0.6809 valid loss: 7.0328, acc 0.7052\n",
      "epoch  24: train loss : 7.0682, acc 0.6878 valid loss: 7.4069, acc 0.7070\n",
      "epoch  25: train loss : 7.0044, acc 0.6916 valid loss: 6.1603, acc 0.7294\n",
      "epoch  26: train loss : 6.9985, acc 0.6953 valid loss: 6.6735, acc 0.7262\n",
      "epoch  27: train loss : 6.8454, acc 0.7056 valid loss: 6.6633, acc 0.7105\n",
      "epoch  28: train loss : 6.7317, acc 0.7062 valid loss: 6.9519, acc 0.7420\n",
      "epoch  29: train loss : 6.6041, acc 0.7143 valid loss: 6.3525, acc 0.7376\n",
      "epoch  30: train loss : 6.5117, acc 0.7157 valid loss: 7.2814, acc 0.7125\n",
      "epoch  31: train loss : 6.5392, acc 0.7111 valid loss: 6.3020, acc 0.7300\n",
      "epoch  32: train loss : 6.4713, acc 0.7164 valid loss: 7.1725, acc 0.7248\n",
      "epoch  33: train loss : 6.2940, acc 0.7258 valid loss: 6.1051, acc 0.7402\n",
      "epoch  34: train loss : 6.3274, acc 0.7230 valid loss: 6.0442, acc 0.7475\n",
      "epoch  35: train loss : 6.2692, acc 0.7239 valid loss: 6.1904, acc 0.7469\n",
      "epoch  36: train loss : 6.2096, acc 0.7254 valid loss: 6.4245, acc 0.7297\n",
      "epoch  37: train loss : 6.1656, acc 0.7248 valid loss: 6.8214, acc 0.7274\n",
      "epoch  38: train loss : 6.1251, acc 0.7309 valid loss: 5.6930, acc 0.7504\n",
      "epoch  39: train loss : 6.0336, acc 0.7334 valid loss: 6.2150, acc 0.7294\n",
      "epoch  40: train loss : 5.9591, acc 0.7446 valid loss: 6.8880, acc 0.7359\n",
      "epoch  41: train loss : 5.8574, acc 0.7421 valid loss: 6.5233, acc 0.7478\n",
      "epoch  42: train loss : 5.8534, acc 0.7476 valid loss: 5.8365, acc 0.7583\n",
      "epoch  43: train loss : 5.8508, acc 0.7408 valid loss: 5.5265, acc 0.7636\n",
      "epoch  44: train loss : 5.7963, acc 0.7541 valid loss: 6.2296, acc 0.7531\n",
      "epoch  45: train loss : 5.7469, acc 0.7502 valid loss: 5.7938, acc 0.7636\n",
      "epoch  46: train loss : 5.7113, acc 0.7487 valid loss: 5.9469, acc 0.7464\n",
      "epoch  47: train loss : 5.6260, acc 0.7631 valid loss: 5.6337, acc 0.7665\n",
      "epoch  48: train loss : 5.6340, acc 0.7549 valid loss: 5.6442, acc 0.7571\n",
      "epoch  49: train loss : 5.6360, acc 0.7548 valid loss: 5.8788, acc 0.7598\n",
      "epoch  50: train loss : 5.4632, acc 0.7615 valid loss: 6.1797, acc 0.7481\n",
      "epoch  51: train loss : 5.4353, acc 0.7634 valid loss: 5.3178, acc 0.7606\n",
      "epoch  52: train loss : 5.2651, acc 0.7673 valid loss: 5.6514, acc 0.7650\n",
      "epoch  53: train loss : 5.4903, acc 0.7647 valid loss: 8.2061, acc 0.7195\n",
      "epoch  54: train loss : 5.4342, acc 0.7665 valid loss: 5.4429, acc 0.7706\n",
      "epoch  55: train loss : 5.3273, acc 0.7705 valid loss: 5.4682, acc 0.7659\n",
      "epoch  56: train loss : 5.2866, acc 0.7715 valid loss: 5.5076, acc 0.7741\n",
      "epoch  57: train loss : 5.2764, acc 0.7758 valid loss: 5.7760, acc 0.7668\n",
      "epoch  58: train loss : 5.1667, acc 0.7777 valid loss: 5.5042, acc 0.7609\n",
      "epoch  59: train loss : 5.2049, acc 0.7743 valid loss: 5.5023, acc 0.7571\n",
      "epoch  60: train loss : 5.1034, acc 0.7785 valid loss: 5.2371, acc 0.7656\n",
      "epoch  61: train loss : 5.0810, acc 0.7830 valid loss: 5.9820, acc 0.7612\n",
      "epoch  62: train loss : 5.0660, acc 0.7841 valid loss: 5.6290, acc 0.7563\n",
      "epoch  63: train loss : 5.0897, acc 0.7842 valid loss: 6.1539, acc 0.7513\n",
      "epoch  64: train loss : 4.9529, acc 0.7928 valid loss: 5.7364, acc 0.7598\n",
      "epoch  65: train loss : 4.9562, acc 0.7886 valid loss: 5.4643, acc 0.7694\n",
      "epoch  66: train loss : 5.0084, acc 0.7865 valid loss: 5.1452, acc 0.7781\n",
      "epoch  67: train loss : 4.9001, acc 0.7910 valid loss: 5.2227, acc 0.7796\n",
      "epoch  68: train loss : 4.9412, acc 0.7845 valid loss: 5.7605, acc 0.7770\n",
      "epoch  69: train loss : 4.8649, acc 0.7880 valid loss: 4.8014, acc 0.7875\n",
      "epoch  70: train loss : 4.7602, acc 0.7980 valid loss: 5.5179, acc 0.7697\n",
      "epoch  71: train loss : 4.8833, acc 0.7947 valid loss: 5.1567, acc 0.7886\n",
      "epoch  72: train loss : 4.7997, acc 0.7943 valid loss: 4.9991, acc 0.7831\n",
      "epoch  73: train loss : 4.7899, acc 0.7998 valid loss: 5.2173, acc 0.7837\n",
      "epoch  74: train loss : 4.7385, acc 0.7938 valid loss: 5.2873, acc 0.7752\n",
      "epoch  75: train loss : 4.6475, acc 0.7996 valid loss: 4.6282, acc 0.7962\n",
      "epoch  76: train loss : 4.6817, acc 0.8052 valid loss: 5.6126, acc 0.7880\n",
      "epoch  77: train loss : 4.7199, acc 0.7996 valid loss: 5.2653, acc 0.7813\n",
      "epoch  78: train loss : 4.6332, acc 0.8004 valid loss: 5.1993, acc 0.7738\n",
      "epoch  79: train loss : 4.6097, acc 0.8024 valid loss: 5.2017, acc 0.7886\n",
      "epoch  80: train loss : 4.5695, acc 0.8085 valid loss: 5.1113, acc 0.7930\n",
      "epoch  81: train loss : 4.6032, acc 0.8064 valid loss: 5.1592, acc 0.7921\n",
      "epoch  82: train loss : 4.6096, acc 0.8093 valid loss: 4.9995, acc 0.7880\n",
      "epoch  83: train loss : 4.5383, acc 0.8042 valid loss: 4.8060, acc 0.8055\n",
      "epoch  84: train loss : 4.4933, acc 0.8131 valid loss: 5.2080, acc 0.7840\n",
      "epoch  85: train loss : 4.5695, acc 0.8086 valid loss: 5.5076, acc 0.7767\n",
      "epoch  86: train loss : 4.4456, acc 0.8106 valid loss: 4.9164, acc 0.7828\n",
      "epoch  87: train loss : 4.4877, acc 0.8104 valid loss: 4.9019, acc 0.7910\n",
      "epoch  88: train loss : 4.4086, acc 0.8086 valid loss: 5.3956, acc 0.7685\n",
      "epoch  89: train loss : 4.4630, acc 0.8173 valid loss: 5.0098, acc 0.7962\n",
      "epoch  90: train loss : 4.3632, acc 0.8163 valid loss: 4.7886, acc 0.7950\n",
      "epoch  91: train loss : 4.4783, acc 0.8160 valid loss: 5.1129, acc 0.7866\n",
      "epoch  92: train loss : 4.3295, acc 0.8118 valid loss: 4.9153, acc 0.8009\n",
      "epoch  93: train loss : 4.3119, acc 0.8218 valid loss: 4.5843, acc 0.7945\n",
      "epoch  94: train loss : 4.3126, acc 0.8169 valid loss: 4.7278, acc 0.7980\n",
      "epoch  95: train loss : 4.3642, acc 0.8204 valid loss: 4.9686, acc 0.7904\n",
      "epoch  96: train loss : 4.3131, acc 0.8212 valid loss: 4.9055, acc 0.7886\n",
      "epoch  97: train loss : 4.3176, acc 0.8185 valid loss: 5.6487, acc 0.7895\n",
      "epoch  98: train loss : 4.2525, acc 0.8194 valid loss: 5.3123, acc 0.7770\n",
      "epoch  99: train loss : 4.2325, acc 0.8282 valid loss: 4.8619, acc 0.7904\n",
      "epoch 100: train loss : 4.2883, acc 0.8232 valid loss: 4.6038, acc 0.7930\n",
      "epoch 101: train loss : 4.2723, acc 0.8244 valid loss: 4.9072, acc 0.7907\n",
      "epoch 102: train loss : 4.2380, acc 0.8284 valid loss: 5.1983, acc 0.7758\n",
      "epoch 103: train loss : 4.2069, acc 0.8275 valid loss: 6.0998, acc 0.7799\n",
      "epoch 104: train loss : 4.1996, acc 0.8289 valid loss: 4.8146, acc 0.7816\n",
      "epoch 105: train loss : 4.1532, acc 0.8264 valid loss: 5.0961, acc 0.7936\n",
      "epoch 106: train loss : 4.2125, acc 0.8320 valid loss: 5.1774, acc 0.7863\n",
      "epoch 107: train loss : 4.2089, acc 0.8279 valid loss: 4.9788, acc 0.7913\n",
      "epoch 108: train loss : 4.2037, acc 0.8255 valid loss: 4.9216, acc 0.7857\n",
      "epoch 109: train loss : 4.1736, acc 0.8237 valid loss: 5.0548, acc 0.7971\n",
      "epoch 110: train loss : 4.0933, acc 0.8310 valid loss: 5.2222, acc 0.7974\n",
      "epoch 111: train loss : 4.1643, acc 0.8302 valid loss: 5.1444, acc 0.7904\n",
      "epoch 112: train loss : 4.1375, acc 0.8385 valid loss: 5.2563, acc 0.7913\n",
      "epoch 113: train loss : 4.1010, acc 0.8274 valid loss: 4.8202, acc 0.8015\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-a5256a589d8f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mupdate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mstudent_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0mvalid_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalid_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_dataloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mupdate\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[1;33m>\u001b[0m\u001b[0mnow_best_acc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-a5256a589d8f>\u001b[0m in \u001b[0;36mrun_epoch\u001b[1;34m(dataloader, update, alpha)\u001b[0m\n\u001b[0;32m     19\u001b[0m                 \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstudent_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn_kd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhard_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msoft_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mtotal_hit\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mhard_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mtotal_num\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#inference  预测"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.3",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit"
  },
  "interpreter": {
   "hash": "0843aa2147bb7b68e1331c060614b1ebfeaba0f0db744f4b489daeb337a1f0b2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}