{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bert模型训练二分类模型 \n",
    "\n",
    "数据集（fake and real news dataset）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import 相关的库\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import pandas as pd\n",
    "import random, time\n",
    "from babel.dates import format_date, format_datetime, format_time\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import transformers, os\n",
    "from transformers import BertModel, AutoModel, AdamW, get_linear_schedule_with_warmup, BertTokenizer, BertForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#empty cache  \n",
    "torch.cuda.empty_cache()\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 1234\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data and make dataset\n",
    "df1 = pd.read_csv('D:/ML_data_sql/news/True.csv')\n",
    "df2 = pd.read_csv('D:/ML_data_sql/news/Fake.csv')\n",
    "df1['label'] = 1\n",
    "df2['label'] = 0\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "#data clean\n",
    "def cleandata(data):\n",
    "    data.text = data.text.str.replace('[#,@,&]','')\n",
    "    data.text = data.text.str.replace('\\d*','')\n",
    "    data.text = data.text.str.replace('w{3}','')\n",
    "    data.text = data.text.str.replace('http\\S+','')\n",
    "    data.text = data.text.str.replace('\\s+','')\n",
    "    data.text = data.text.str.replace(r'\\s+[a-zA-Z]\\s+','')\n",
    "    #set stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words.extend(['u','wa','ha','would','com'])\n",
    "    data['text'] = data['text'].apply(lambda x: \" \".join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "cleandata(df)\n",
    "    \n",
    "#split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2, random_state=seed_val, shuffle=True)\n",
    "X_train_Transformer, X_val_Transformer, y_train_Transformer, y_val_Transformer = train_test_split(\n",
    "                                                    X_train, y_train, test_size=0.20, random_state=seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download pretrained model settings\n",
    "model_name = 'bert-base-cased'  #TODO\n",
    "SEQ_LEN = 200\n",
    "batch_size = 2 \n",
    "epochs = 1\n",
    "learning_rate = 1e-5 # Controls how large a step is taken when updating model weights during training.\n",
    "steps_per_epoch = 100\n",
    "num_workers = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the sentence 200 word  a sentence\n",
    "def split_sentence(sentence):\n",
    "    l_total = []\n",
    "    l_parcial = []\n",
    "    if len(sentence.split())//SEQ_LEN>0:\n",
    "        n = len(sentence.split())//SEQ_LEN\n",
    "    else:\n",
    "        n =1\n",
    "    for i in range(n):\n",
    "        if i ==0:\n",
    "            l_parcial = sentence.split()[:SEQ_LEN]\n",
    "            l_total.append(\" \".join(l_parcial))\n",
    "        else:\n",
    "            l_parcial = sentence.split()[i*SEQ_LEN:(i+1)*SEQ_LEN]\n",
    "            l_total.append(\" \".join(l_parcial))\n",
    "    return str(l_total)\n",
    "# Splits train and validation sets to be feed to the transformer which only accepts 512 tokens maximum\n",
    "split_train_text = [split_sentence(t) for t in X_train_Transformer]\n",
    "split_valid_text = [split_sentence(t) for t in X_val_Transformer]\n",
    "split_test_text = [split_sentence(t) for t in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split_valid_text[0],len(split_valid_text[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load tokenizer\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name, do_lower_case=True)\n",
    "def encoding(text):\n",
    "    #encode text\n",
    "    encoded_text = tokenizer.batch_encode_plus(\n",
    "        list(text),\n",
    "        max_length=SEQ_LEN,\n",
    "        add_special_tokens=True, # Add '[CLS]' and '[SEP]'#使用bert模型必须要有这两个 一个在句头一个在句尾\n",
    "        return_token_type_ids=True,\n",
    "        truncation=True,\n",
    "        padding='longest',\n",
    "        return_attention_mask=True,)\n",
    "    return encoded_text\n",
    "\n",
    "traincoding = encoding(split_train_text)\n",
    "validcoding = encoding(split_valid_text)\n",
    "testcoding = encoding(split_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(traincoding.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#load the weights\n",
    "class_wts = compute_class_weight('balanced', np.unique(df['label'].values.tolist()), \n",
    "                                 df['label'])\n",
    "\n",
    "# convert class weights to tensor\n",
    "weights= torch.tensor(class_wts,dtype=torch.float)\n",
    "weights = weights.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the train data \n",
    "def makeDataloader(coding,transformers,batch_size,num_workers):\n",
    "    coding_seq  = torch.tensor(coding['input_ids'])\n",
    "    coding_mask = torch.tensor(coding['attention_mask'])\n",
    "    coding_token_ids = torch.tensor(coding['token_type_ids'])\n",
    "    coding_label = torch.tensor(transformers.tolist())\n",
    "    # wrap the data to dataloader\n",
    "    dataset = TensorDataset(coding_seq, coding_mask, coding_token_ids, coding_label)\n",
    "    sampler = RandomSampler(dataset)\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        sampler=sampler,\n",
    "        pin_memory=True\n",
    "    ),coding_label\n",
    "\n",
    "traindata,train_label = makeDataloader(traincoding,y_train_Transformer,batch_size,num_workers)\n",
    "valdata,val_label = makeDataloader(validcoding,y_val_Transformer,batch_size,num_workers)\n",
    "testdata,test_label =makeDataloader(testcoding,y_test,batch_size,num_workers)\n",
    "\n",
    "\n",
    "print('Number of data in the train set', len(traindata),)\n",
    "print('Number of data in the validation set', len(valdata))\n",
    "print('Number of data in the test set', len(testdata))\n",
    "#如果太多 应使用一部分数据进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load bert model\n",
    "class BERT_Arch(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes, freeze_bert=False):\n",
    "        \n",
    "        super(BERT_Arch,self).__init__()\n",
    "        # Instantiating BERT model object\n",
    "        self.bert = BertModel.from_pretrained(model_name, return_dict=False)\n",
    "        \n",
    "        # Freeze bert layers\n",
    "        if freeze_bert:\n",
    "            for p in self.bert.parameters():\n",
    "                p.requires_grad = False\n",
    "                \n",
    "        self.bert_drop_1 = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, self.bert.config.hidden_size) # (768, 64)\n",
    "        self.bn = nn.BatchNorm1d(768) # (768)\n",
    "        self.bert_drop_2 = nn.Dropout(0.25)\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, n_classes) # (768,2)\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        _, output = self.bert(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask = attention_mask,\n",
    "            token_type_ids = token_type_ids\n",
    "        )\n",
    "        output = self.bert_drop_1(output)\n",
    "        output = self.fc(output)\n",
    "        output = self.bn(output)\n",
    "        output = self.bert_drop_2(output)\n",
    "        output = self.out(output)        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#traing settings\n",
    "print('Downloading the BERT custom model...')\n",
    "#2分类\n",
    "model = BERT_Arch(2)\n",
    "model.to(device)\n",
    "cross_entropy  = nn.CrossEntropyLoss(weight=weights)\n",
    "#easy optimizer \n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    print('training... ')\n",
    "    model.train()\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    # empty list to save model predictions\n",
    "    total_preds=[]\n",
    "    # iterate over batches\n",
    "    for step, batch in enumerate(traindata):\n",
    "    \n",
    "        # progress update after every 100 batches.\n",
    "        if step % 100 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(traindata)))\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            # push the batch to gpu\n",
    "            batch = [r.to(device) for r in batch]\n",
    "\n",
    "        sent_id, mask, token_type_ids, labels = batch\n",
    "        # clear previously calculated gradients \n",
    "        model.zero_grad()        \n",
    "        # get model predictions for the current batch\n",
    "        preds = model(sent_id, mask, token_type_ids)\n",
    "        # compute the loss between actual and predicted values\n",
    "        loss = cross_entropy(preds, labels)\n",
    "        # add on to the total loss\n",
    "        total_loss = total_loss + loss.item()\n",
    "        # backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        # model predictions are stored on GPU. So, push it to CPU\n",
    "        preds=preds.detach().cpu().numpy()\n",
    "        # append the model predictions\n",
    "        total_preds.append(preds)\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(traindata)\n",
    "\n",
    "    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    #returns the loss and predictions\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluating the model \n",
    "def evaluate():\n",
    "  \n",
    "    print(\"\\nEvaluating...\")\n",
    "    #t0 = time.time()\n",
    "    \n",
    "    model.eval() # deactivate dropout layers\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    \n",
    "    # empty list to save the model predictions\n",
    "    total_preds = []\n",
    "\n",
    "    # iterate over batches\n",
    "    for step, batch in enumerate(valdata):\n",
    "        # Progress update every 100 batches.\n",
    "        if step % 100 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            #elapsed = format_time(time.time() - t0)\n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(valdata)))\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            # push the batch to gpu\n",
    "            batch = [t.to(device) for t in batch]\n",
    "\n",
    "        sent_id, mask, token_type_ids, labels = batch\n",
    "\n",
    "        # deactivate autograd\n",
    "        with torch.no_grad(): # Dont store any previous computations, thus freeing GPU space\n",
    "\n",
    "            # model predictions\n",
    "            preds = model(sent_id, mask, token_type_ids)\n",
    "            # compute the validation loss between actual and predicted values\n",
    "            loss = cross_entropy(preds, labels)\n",
    "            total_loss = total_loss + loss.item()\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "            total_preds.append(preds)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "    # compute the validation loss of the epoch\n",
    "    avg_loss = total_loss / len(valdata) \n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_valid_loss = float('inf')\n",
    "\n",
    "# Empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "# for each epoch perform training and evaluation\n",
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "    #train model\n",
    "    train_loss, train_pred = train()\n",
    "    \n",
    "    #evaluate model\n",
    "    valid_loss, val_pred = evaluate()\n",
    "    \n",
    "    print('Evaluation done for epoch {}'.format(epoch + 1))\n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        print('Saving model...')\n",
    "        torch.save(model.state_dict(), 'bert_weights.pth') # Save model weight's (you can also save it in .bin format)\n",
    "   # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    #compute the acc\n",
    "    train_acc = accuracy_score(train_pred, train_label)\n",
    "    val_acc = accuracy_score(val_pred, val_label)\n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}, Training Accuracy: {train_acc:.4f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}, Validation Accuracy: {val_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #predict\n",
    "print('\\nTest Set...')\n",
    "\n",
    "test_preds = []\n",
    "\n",
    "print('Total batches:', len(testdata))\n",
    "\n",
    "for fold_index in range(0, 3):\n",
    "    \n",
    "    print('\\nFold Model', fold_index)\n",
    "    \n",
    "    # Load the fold model\n",
    "    path_model = 'bert_weights.pt'\n",
    "    model.load_state_dict(torch.load(path_model))\n",
    "\n",
    "    # Send the model to the GPU\n",
    "    model.to(device)\n",
    "\n",
    "    stacked_val_labels = []\n",
    "    \n",
    "    # Put the model in evaluation mode.\n",
    "    model.eval()\n",
    "\n",
    "    # Turn off the gradient calculations.\n",
    "    # This tells the model not to compute or store gradients.\n",
    "    # This step saves memory and speeds up validation.\n",
    "    torch.set_grad_enabled(False)\n",
    "\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_val_loss = 0\n",
    "\n",
    "    for j, test_batch in enumerate(testdata):\n",
    "\n",
    "        inference_status = 'Batch ' + str(j + 1)\n",
    "\n",
    "        print(inference_status, end='\\r')\n",
    "\n",
    "        b_input_ids = test_batch[0].to(device)\n",
    "        b_input_mask = test_batch[1].to(device)\n",
    "        b_token_type_ids = test_batch[2].to(device)\n",
    "        b_test_y = test_batch[3].to(device)\n",
    "\n",
    "\n",
    "        outputs = model(b_input_ids, \n",
    "                        attention_mask=b_input_mask,\n",
    "                        token_type_ids=b_token_type_ids)\n",
    "\n",
    "        # Get the preds\n",
    "        preds = outputs[0]\n",
    "\n",
    "        # Move preds to the CPU\n",
    "        val_preds = preds.detach().cpu().numpy()\n",
    "        acc = accuracy_score(val_preds, b_test_y)\n",
    "        #true_labels.append(b_test_y.to('cpu').numpy().flatten())\n",
    "        print(acc)\n",
    "        # Stack the predictions.\n",
    "        if j == 0:  # first batch\n",
    "            stacked_val_preds = val_preds\n",
    "            \n",
    "        else:\n",
    "            stacked_val_preds = np.vstack((stacked_val_preds, val_preds))\n",
    "            \n",
    "    test_preds.append(stacked_val_preds)\n",
    "    \n",
    "            \n",
    "print('\\nPrediction complete.')\n",
    "for i, item in enumerate(test_preds):\n",
    "    if i == 0:\n",
    "        preds = item\n",
    "    else:\n",
    "        # Sum the matrices\n",
    "        preds = item + preds\n",
    "\n",
    "# Average the predictions\n",
    "avg_preds = preds/(len(test_preds))\n",
    "\n",
    "#print(preds)\n",
    "#print()\n",
    "#print(avg_preds)\n",
    "\n",
    "# Take the argmax. \n",
    "# This returns the column index of the max value in each row.\n",
    "test_predictions = np.argmax(avg_preds, axis=1)\n",
    "\n",
    "# Take a look of the output\n",
    "print(type(test_predictions))\n",
    "print(len(test_predictions))\n",
    "print()\n",
    "print(test_predictions)\n",
    "\n",
    "true_y = []\n",
    "for j, test_batch in enumerate(testdata):\n",
    "    true_y.append(int(test_batch[3][0].numpy().flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualtion\n",
    "\n",
    "target_names = ['true_y', 'predicted_y']\n",
    "\n",
    "data = {'true_y': true_y,\n",
    "       'predicted_y': test_predictions}\n",
    "\n",
    "df_pred_BERT = pd.DataFrame(data, columns=['true_y','predicted_y'])\n",
    "\n",
    "confusion_matrix = pd.crosstab(df_pred_BERT['true_y'], df_pred_BERT['predicted_y'], rownames=['True'], colnames=['Predicted'])\n",
    "\n",
    "sns.heatmap(confusion_matrix, annot=True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0843aa2147bb7b68e1331c060614b1ebfeaba0f0db744f4b489daeb337a1f0b2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
