{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import pandas as pd\n",
    "import random, time\n",
    "from babel.dates import format_date, format_datetime, format_time\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import transformers, os\n",
    "from transformers import BertModel, AutoModel, AdamW, get_linear_schedule_with_warmup, BertTokenizer, BertForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "# Get the GPU device name if available.\n",
    "if torch.cuda.is_available():    \n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are {} GPU(s) available.'.format(torch.cuda.device_count()))\n",
    "    print('We will use the GPU: {}'.format(torch.cuda.get_device_name(0)))\n",
    "\n",
    "# If we dont have GPU but a CPU, training will take place on CPU instead\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "torch.cuda.empty_cache()\n",
    "    \n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('D:/ML_data_sql/news/True.csv')\n",
    "df2 = pd.read_csv('D:/ML_data_sql/news/Fake.csv')\n",
    "df1['label'] = 1\n",
    "df2['label'] = 0\n",
    "df = pd.concat([df1, df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the data \n",
    "df.text = df.text.str.replace('[#,@,&]', '')\n",
    "# Remove digits\n",
    "df.text = df.text.str.replace('\\d*','')\n",
    "#Remove www\n",
    "df.text = df.text.str.replace('w{3}','')\n",
    "# remove urls\n",
    "df.text = df.text.str.replace(\"http\\S+\", \"\")\n",
    "# remove multiple spaces with single space\n",
    "df.text = df.text.str.replace('\\s+', ' ')\n",
    "#remove all single characters\n",
    "df.text = df.text.str.replace(r'\\s+[a-zA-Z]\\s+', '')\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['u', 'wa', 'ha', 'would', 'com'])\n",
    "# Remove english stopwords\n",
    "df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split train and test\n",
    "# Split test and train data using 25% of the dataset for validation purposes\n",
    "x_train, x_test, y_train, y_test = train_test_split(df['text'], \n",
    "                                                      df['label'], test_size=0.25, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain a 10% test set from train set\n",
    "X_train_Transformer, X_val_Transformer, y_train_Transformer, y_val_Transformer = train_test_split(\n",
    "                                                    x_train, y_train, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#settings \n",
    "model_name = 'bert-base-cased'\n",
    "SEQ_LEN = 200\n",
    "batch_size = 2 \n",
    "epochs = 5\n",
    "learning_rate = 1e-5 # Controls how large a step is taken when updating model weights during training.\n",
    "steps_per_epoch = 50\n",
    "num_workers = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#划分 一下 句子 150 个词 为一个\n",
    "def get_split(text1):\n",
    "    '''Get split of the text with 200 char lenght'''\n",
    "    l_total = []\n",
    "    l_parcial = []\n",
    "    if len(text1.split())//150 >0:\n",
    "        n = len(text1.split())//150\n",
    "    else: \n",
    "        n = 1\n",
    "    for w in range(n):\n",
    "        if w == 0:\n",
    "            l_parcial = text1.split()[:200]\n",
    "            l_total.append(\" \".join(l_parcial))\n",
    "        else:\n",
    "            l_parcial = text1.split()[w*150:w*150 + 200]\n",
    "            l_total.append(\" \".join(l_parcial))\n",
    "    return str(l_total)\n",
    "\n",
    "# Splits train and validation sets to be feed to the transformer which only accepts 512 tokens maximum\n",
    "split_train_text = [get_split(t) for t in X_train_Transformer]\n",
    "split_valid_text = [get_split(t) for t in X_val_Transformer]\n",
    "split_test_text = [get_split(t) for t in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['Here epic response criticism: Lady came told meshould take hat off. Told go get money back ugly haircut. That ended THAT! Mic drop! Lady came told meshould take hat off. Told go get money back ugly haircut. That ended THAT! Mic drop! pic.twitter.com/SfQQDMNiV David A. Clarke Jr. (SheriffClarke) February']\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_valid_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 208k/208k [00:00<00:00, 272kB/s] \n",
      "Downloading: 100%|██████████| 29.0/29.0 [00:00<00:00, 7.26kB/s]\n",
      "Downloading: 100%|██████████| 426k/426k [00:05<00:00, 76.4kB/s] \n",
      "Downloading: 100%|██████████| 570/570 [00:00<00:00, 285kB/s]\n"
     ]
    }
   ],
   "source": [
    "# Load the RoBERTa tokenizer and tokenize the data\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding \n",
    "trencoding = tokenizer.batch_encode_plus(\n",
    "  list(split_train_text),\n",
    "  max_length=SEQ_LEN,\n",
    "  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
    "  return_token_type_ids=True,\n",
    "  truncation=True,\n",
    "  padding='longest',\n",
    "  return_attention_mask=True,\n",
    ")\n",
    "\n",
    "valencoding = tokenizer.batch_encode_plus(\n",
    "  list(split_valid_text),\n",
    "  max_length=SEQ_LEN,\n",
    "  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
    "  return_token_type_ids=True,\n",
    "  truncation=True,\n",
    "  padding='longest',\n",
    "  return_attention_mask=True,\n",
    ")\n",
    "\n",
    "\n",
    "testencoding = tokenizer.batch_encode_plus(\n",
    "  list(split_test_text),\n",
    "  max_length=SEQ_LEN,\n",
    "  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
    "  return_token_type_ids=True,\n",
    "  truncation=True,\n",
    "  padding='longest',\n",
    "  return_attention_mask=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unk_token': '[UNK]',\n",
       " 'sep_token': '[SEP]',\n",
       " 'pad_token': '[PAD]',\n",
       " 'cls_token': '[CLS]',\n",
       " 'mask_token': '[MASK]'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trencoding.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find Class Weights\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class_wts = compute_class_weight('balanced', np.unique(df['label'].values.tolist()), \n",
    "                                 df['label'])\n",
    "\n",
    "#print(class_wts)\n",
    "\n",
    "# convert class weights to tensor\n",
    "weights= torch.tensor(class_wts,dtype=torch.float)\n",
    "weights = weights.to(device)\n",
    "\n",
    "# loss function\n",
    "#cross_entropy  = nn.NLLLoss(weight=weights) \n",
    "cross_entropy  = nn.CrossEntropyLoss(weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data in the train set 13469\n",
      "Number of data in the validation set 3368\n",
      "Number of data in the test set 5613\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "def loadData(prep_df, batch_size, num_workers, sampler):\n",
    "    \n",
    "    return  DataLoader(\n",
    "            prep_df,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=num_workers,\n",
    "            sampler=sampler,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "## convert lists to tensors\n",
    "train_seq = torch.tensor(trencoding['input_ids'])\n",
    "train_mask = torch.tensor(trencoding['attention_mask'])\n",
    "train_token_ids = torch.tensor(trencoding['token_type_ids'])\n",
    "train_y = torch.tensor(y_train_Transformer.tolist())\n",
    "\n",
    "val_seq = torch.tensor(valencoding['input_ids'])\n",
    "val_mask = torch.tensor(valencoding['attention_mask'])\n",
    "val_token_ids = torch.tensor(valencoding['token_type_ids'])\n",
    "val_y = torch.tensor(y_val_Transformer.tolist())\n",
    "\n",
    "test_seq = torch.tensor(testencoding['input_ids'])\n",
    "test_mask = torch.tensor(testencoding['attention_mask'])\n",
    "test_token_ids = torch.tensor(testencoding['token_type_ids'])\n",
    "test_y = torch.tensor(y_test.tolist())\n",
    "\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_token_ids, train_y)\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "# Train Data Loader\n",
    "traindata = loadData(train_data, batch_size, num_workers, train_sampler)\n",
    "\n",
    "# wrap tensors\n",
    "val_data = TensorDataset(val_seq, val_mask, val_token_ids, val_y)\n",
    "# sampler for sampling the data during training\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "# Val Data Loader\n",
    "valdata = loadData(val_data, batch_size, num_workers, val_sampler)\n",
    "\n",
    "# wrap tensors\n",
    "test_data = TensorDataset(test_seq, test_mask, test_token_ids, test_y)\n",
    "# sampler for sampling the data during training\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "# Val Data Loader\n",
    "testdata = loadData(test_data, batch_size, num_workers, test_sampler)\n",
    "\n",
    "\n",
    "print('Number of data in the train set', len(traindata),)\n",
    "print('Number of data in the validation set', len(valdata))\n",
    "print('Number of data in the test set', len(testdata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load bert model\n",
    "class BERT_Arch(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes, freeze_bert=False):\n",
    "        \n",
    "        super(BERT_Arch,self).__init__()\n",
    "        # Instantiating BERT model object\n",
    "        self.bert = BertModel.from_pretrained(model_name, return_dict=False)\n",
    "        \n",
    "        # Freeze bert layers\n",
    "        if freeze_bert:\n",
    "            for p in self.bert.parameters():\n",
    "                p.requires_grad = False\n",
    "                \n",
    "        self.bert_drop_1 = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, self.bert.config.hidden_size) # (768, 64)\n",
    "        self.bn = nn.BatchNorm1d(768) # (768)\n",
    "        self.bert_drop_2 = nn.Dropout(0.25)\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, n_classes) # (768,2)\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        _, output = self.bert(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask = attention_mask,\n",
    "            token_type_ids = token_type_ids\n",
    "        )\n",
    "        output = self.bert_drop_1(output)\n",
    "        output = self.fc(output)\n",
    "        output = self.bn(output)\n",
    "        output = self.bert_drop_2(output)\n",
    "        output = self.out(output)        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading the BERT custom model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 416M/416M [02:51<00:00, 2.55MB/s] \n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing the optimizer...\n"
     ]
    }
   ],
   "source": [
    "class_names = np.unique(df['label'])\n",
    "print('Downloading the BERT custom model...')\n",
    "model = BERT_Arch(len(class_names))\n",
    "model.to(device)\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "optimizer_parameters = [{'params': [p for n, p in param_optimizer \n",
    "                                    if not any(nd in n for nd in no_decay)],'weight_decay':0.001},\n",
    "                        {'params': [p for n, p in param_optimizer \n",
    "                                    if any(nd in n for nd in no_decay)],'weight_decay':0.0}]\n",
    "\n",
    "print('Preparing the optimizer...')\n",
    "#optimizer \n",
    "optimizer = AdamW(optimizer_parameters, lr=learning_rate)\n",
    "steps = steps_per_epoch\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps = 0,\n",
    "    num_training_steps = steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the bert model\n",
    "def trainBERT():\n",
    "  \n",
    "    print('Training...')\n",
    "    model.train()\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "\n",
    "    # empty list to save model predictions\n",
    "    total_preds=[]\n",
    "\n",
    "    # iterate over batches\n",
    "    for step, batch in enumerate(traindata):\n",
    "    \n",
    "        # progress update after every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(traindata)))\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            # push the batch to gpu\n",
    "            batch = [r.to(device) for r in batch]\n",
    "\n",
    "        sent_id, mask, token_type_ids, labels = batch\n",
    "        # clear previously calculated gradients \n",
    "        model.zero_grad()        \n",
    "        # get model predictions for the current batch\n",
    "        preds = model(sent_id, mask, token_type_ids)\n",
    "        # compute the loss between actual and predicted values\n",
    "        loss = cross_entropy(preds, labels)\n",
    "        # add on to the total loss\n",
    "        total_loss = total_loss + loss.item()\n",
    "        # backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        # model predictions are stored on GPU. So, push it to CPU\n",
    "        preds=preds.detach().cpu().numpy()\n",
    "        # append the model predictions\n",
    "        total_preds.append(preds)\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(traindata)\n",
    "\n",
    "    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    #returns the loss and predictions\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluating the model \n",
    "def evaluate():\n",
    "  \n",
    "    print(\"\\nEvaluating...\")\n",
    "    t0 = time.time()\n",
    "    \n",
    "    model.eval() # deactivate dropout layers\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    \n",
    "    # empty list to save the model predictions\n",
    "    total_preds = []\n",
    "\n",
    "    # iterate over batches\n",
    "    for step, batch in enumerate(valdata):\n",
    "        # Progress update every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(valdata)))\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            # push the batch to gpu\n",
    "            batch = [t.to(device) for t in batch]\n",
    "\n",
    "        sent_id, mask, token_type_ids, labels = batch\n",
    "\n",
    "        # deactivate autograd\n",
    "        with torch.no_grad(): # Dont store any previous computations, thus freeing GPU space\n",
    "\n",
    "            # model predictions\n",
    "            preds = model(sent_id, mask, token_type_ids)\n",
    "            # compute the validation loss between actual and predicted values\n",
    "            loss = cross_entropy(preds, labels)\n",
    "            total_loss = total_loss + loss.item()\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "            total_preds.append(preds)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "    # compute the validation loss of the epoch\n",
    "    avg_loss = total_loss / len(valdata) \n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 5\n",
      "Training...\n",
      "  Batch    50  of  13,469.\n",
      "  Batch   100  of  13,469.\n",
      "  Batch   150  of  13,469.\n",
      "  Batch   200  of  13,469.\n",
      "  Batch   250  of  13,469.\n",
      "  Batch   300  of  13,469.\n",
      "  Batch   350  of  13,469.\n",
      "  Batch   400  of  13,469.\n",
      "  Batch   450  of  13,469.\n",
      "  Batch   500  of  13,469.\n",
      "  Batch   550  of  13,469.\n",
      "  Batch   600  of  13,469.\n",
      "  Batch   650  of  13,469.\n",
      "  Batch   700  of  13,469.\n",
      "  Batch   750  of  13,469.\n",
      "  Batch   800  of  13,469.\n",
      "  Batch   850  of  13,469.\n",
      "  Batch   900  of  13,469.\n",
      "  Batch   950  of  13,469.\n",
      "  Batch 1,000  of  13,469.\n",
      "  Batch 1,050  of  13,469.\n",
      "  Batch 1,100  of  13,469.\n",
      "  Batch 1,150  of  13,469.\n",
      "  Batch 1,200  of  13,469.\n",
      "  Batch 1,250  of  13,469.\n",
      "  Batch 1,300  of  13,469.\n",
      "  Batch 1,350  of  13,469.\n",
      "  Batch 1,400  of  13,469.\n",
      "  Batch 1,450  of  13,469.\n",
      "  Batch 1,500  of  13,469.\n",
      "  Batch 1,550  of  13,469.\n",
      "  Batch 1,600  of  13,469.\n",
      "  Batch 1,650  of  13,469.\n",
      "  Batch 1,700  of  13,469.\n",
      "  Batch 1,750  of  13,469.\n",
      "  Batch 1,800  of  13,469.\n",
      "  Batch 1,850  of  13,469.\n",
      "  Batch 1,900  of  13,469.\n",
      "  Batch 1,950  of  13,469.\n",
      "  Batch 2,000  of  13,469.\n",
      "  Batch 2,050  of  13,469.\n",
      "  Batch 2,100  of  13,469.\n",
      "  Batch 2,150  of  13,469.\n",
      "  Batch 2,200  of  13,469.\n",
      "  Batch 2,250  of  13,469.\n",
      "  Batch 2,300  of  13,469.\n",
      "  Batch 2,350  of  13,469.\n",
      "  Batch 2,400  of  13,469.\n",
      "  Batch 2,450  of  13,469.\n",
      "  Batch 2,500  of  13,469.\n",
      "  Batch 2,550  of  13,469.\n",
      "  Batch 2,600  of  13,469.\n",
      "  Batch 2,650  of  13,469.\n",
      "  Batch 2,700  of  13,469.\n",
      "  Batch 2,750  of  13,469.\n",
      "  Batch 2,800  of  13,469.\n",
      "  Batch 2,850  of  13,469.\n",
      "  Batch 2,900  of  13,469.\n",
      "  Batch 2,950  of  13,469.\n",
      "  Batch 3,000  of  13,469.\n",
      "  Batch 3,050  of  13,469.\n",
      "  Batch 3,100  of  13,469.\n",
      "  Batch 3,150  of  13,469.\n",
      "  Batch 3,200  of  13,469.\n",
      "  Batch 3,250  of  13,469.\n",
      "  Batch 3,300  of  13,469.\n",
      "  Batch 3,350  of  13,469.\n",
      "  Batch 3,400  of  13,469.\n",
      "  Batch 3,450  of  13,469.\n",
      "  Batch 3,500  of  13,469.\n",
      "  Batch 3,550  of  13,469.\n",
      "  Batch 3,600  of  13,469.\n",
      "  Batch 3,650  of  13,469.\n",
      "  Batch 3,700  of  13,469.\n",
      "  Batch 3,750  of  13,469.\n",
      "  Batch 3,800  of  13,469.\n",
      "  Batch 3,850  of  13,469.\n",
      "  Batch 3,900  of  13,469.\n",
      "  Batch 3,950  of  13,469.\n",
      "  Batch 4,000  of  13,469.\n",
      "  Batch 4,050  of  13,469.\n",
      "  Batch 4,100  of  13,469.\n",
      "  Batch 4,150  of  13,469.\n",
      "  Batch 4,200  of  13,469.\n",
      "  Batch 4,250  of  13,469.\n",
      "  Batch 4,300  of  13,469.\n",
      "  Batch 4,350  of  13,469.\n",
      "  Batch 4,400  of  13,469.\n",
      "  Batch 4,450  of  13,469.\n",
      "  Batch 4,500  of  13,469.\n",
      "  Batch 4,550  of  13,469.\n",
      "  Batch 4,600  of  13,469.\n",
      "  Batch 4,650  of  13,469.\n",
      "  Batch 4,700  of  13,469.\n",
      "  Batch 4,750  of  13,469.\n",
      "  Batch 4,800  of  13,469.\n",
      "  Batch 4,850  of  13,469.\n",
      "  Batch 4,900  of  13,469.\n",
      "  Batch 4,950  of  13,469.\n",
      "  Batch 5,000  of  13,469.\n",
      "  Batch 5,050  of  13,469.\n",
      "  Batch 5,100  of  13,469.\n",
      "  Batch 5,150  of  13,469.\n",
      "  Batch 5,200  of  13,469.\n",
      "  Batch 5,250  of  13,469.\n",
      "  Batch 5,300  of  13,469.\n",
      "  Batch 5,350  of  13,469.\n",
      "  Batch 5,400  of  13,469.\n",
      "  Batch 5,450  of  13,469.\n",
      "  Batch 5,500  of  13,469.\n",
      "  Batch 5,550  of  13,469.\n",
      "  Batch 5,600  of  13,469.\n",
      "  Batch 5,650  of  13,469.\n",
      "  Batch 5,700  of  13,469.\n",
      "  Batch 5,750  of  13,469.\n",
      "  Batch 5,800  of  13,469.\n",
      "  Batch 5,850  of  13,469.\n",
      "  Batch 5,900  of  13,469.\n",
      "  Batch 5,950  of  13,469.\n",
      "  Batch 6,000  of  13,469.\n",
      "  Batch 6,050  of  13,469.\n",
      "  Batch 6,100  of  13,469.\n",
      "  Batch 6,150  of  13,469.\n",
      "  Batch 6,200  of  13,469.\n",
      "  Batch 6,250  of  13,469.\n",
      "  Batch 6,300  of  13,469.\n",
      "  Batch 6,350  of  13,469.\n",
      "  Batch 6,400  of  13,469.\n",
      "  Batch 6,450  of  13,469.\n",
      "  Batch 6,500  of  13,469.\n",
      "  Batch 6,550  of  13,469.\n",
      "  Batch 6,600  of  13,469.\n",
      "  Batch 6,650  of  13,469.\n",
      "  Batch 6,700  of  13,469.\n",
      "  Batch 6,750  of  13,469.\n",
      "  Batch 6,800  of  13,469.\n",
      "  Batch 6,850  of  13,469.\n",
      "  Batch 6,900  of  13,469.\n",
      "  Batch 6,950  of  13,469.\n",
      "  Batch 7,000  of  13,469.\n",
      "  Batch 7,050  of  13,469.\n",
      "  Batch 7,100  of  13,469.\n",
      "  Batch 7,150  of  13,469.\n",
      "  Batch 7,200  of  13,469.\n",
      "  Batch 7,250  of  13,469.\n",
      "  Batch 7,300  of  13,469.\n",
      "  Batch 7,350  of  13,469.\n",
      "  Batch 7,400  of  13,469.\n",
      "  Batch 7,450  of  13,469.\n",
      "  Batch 7,500  of  13,469.\n",
      "  Batch 7,550  of  13,469.\n",
      "  Batch 7,600  of  13,469.\n",
      "  Batch 7,650  of  13,469.\n",
      "  Batch 7,700  of  13,469.\n",
      "  Batch 7,750  of  13,469.\n",
      "  Batch 7,800  of  13,469.\n",
      "  Batch 7,850  of  13,469.\n",
      "  Batch 7,900  of  13,469.\n",
      "  Batch 7,950  of  13,469.\n",
      "  Batch 8,000  of  13,469.\n",
      "  Batch 8,050  of  13,469.\n",
      "  Batch 8,100  of  13,469.\n",
      "  Batch 8,150  of  13,469.\n",
      "  Batch 8,200  of  13,469.\n",
      "  Batch 8,250  of  13,469.\n",
      "  Batch 8,300  of  13,469.\n",
      "  Batch 8,350  of  13,469.\n",
      "  Batch 8,400  of  13,469.\n",
      "  Batch 8,450  of  13,469.\n",
      "  Batch 8,500  of  13,469.\n",
      "  Batch 8,550  of  13,469.\n",
      "  Batch 8,600  of  13,469.\n",
      "  Batch 8,650  of  13,469.\n",
      "  Batch 8,700  of  13,469.\n",
      "  Batch 8,750  of  13,469.\n",
      "  Batch 8,800  of  13,469.\n",
      "  Batch 8,850  of  13,469.\n",
      "  Batch 8,900  of  13,469.\n",
      "  Batch 8,950  of  13,469.\n",
      "  Batch 9,000  of  13,469.\n",
      "  Batch 9,050  of  13,469.\n",
      "  Batch 9,100  of  13,469.\n",
      "  Batch 9,150  of  13,469.\n",
      "  Batch 9,200  of  13,469.\n",
      "  Batch 9,250  of  13,469.\n",
      "  Batch 9,300  of  13,469.\n",
      "  Batch 9,350  of  13,469.\n",
      "  Batch 9,400  of  13,469.\n",
      "  Batch 9,450  of  13,469.\n",
      "  Batch 9,500  of  13,469.\n",
      "  Batch 9,550  of  13,469.\n",
      "  Batch 9,600  of  13,469.\n",
      "  Batch 9,650  of  13,469.\n",
      "  Batch 9,700  of  13,469.\n",
      "  Batch 9,750  of  13,469.\n",
      "  Batch 9,800  of  13,469.\n",
      "  Batch 9,850  of  13,469.\n",
      "  Batch 9,900  of  13,469.\n",
      "  Batch 9,950  of  13,469.\n",
      "  Batch 10,000  of  13,469.\n",
      "  Batch 10,050  of  13,469.\n",
      "  Batch 10,100  of  13,469.\n",
      "  Batch 10,150  of  13,469.\n",
      "  Batch 10,200  of  13,469.\n",
      "  Batch 10,250  of  13,469.\n",
      "  Batch 10,300  of  13,469.\n",
      "  Batch 10,350  of  13,469.\n",
      "  Batch 10,400  of  13,469.\n",
      "  Batch 10,450  of  13,469.\n",
      "  Batch 10,500  of  13,469.\n",
      "  Batch 10,550  of  13,469.\n",
      "  Batch 10,600  of  13,469.\n",
      "  Batch 10,650  of  13,469.\n",
      "  Batch 10,700  of  13,469.\n",
      "  Batch 10,750  of  13,469.\n",
      "  Batch 10,800  of  13,469.\n",
      "  Batch 10,850  of  13,469.\n",
      "  Batch 10,900  of  13,469.\n",
      "  Batch 10,950  of  13,469.\n",
      "  Batch 11,000  of  13,469.\n",
      "  Batch 11,050  of  13,469.\n",
      "  Batch 11,100  of  13,469.\n",
      "  Batch 11,150  of  13,469.\n",
      "  Batch 11,200  of  13,469.\n",
      "  Batch 11,250  of  13,469.\n",
      "  Batch 11,300  of  13,469.\n",
      "  Batch 11,350  of  13,469.\n",
      "  Batch 11,400  of  13,469.\n",
      "  Batch 11,450  of  13,469.\n",
      "  Batch 11,500  of  13,469.\n",
      "  Batch 11,550  of  13,469.\n",
      "  Batch 11,600  of  13,469.\n",
      "  Batch 11,650  of  13,469.\n",
      "  Batch 11,700  of  13,469.\n",
      "  Batch 11,750  of  13,469.\n",
      "  Batch 11,800  of  13,469.\n",
      "  Batch 11,850  of  13,469.\n",
      "  Batch 11,900  of  13,469.\n",
      "  Batch 11,950  of  13,469.\n",
      "  Batch 12,000  of  13,469.\n",
      "  Batch 12,050  of  13,469.\n",
      "  Batch 12,100  of  13,469.\n",
      "  Batch 12,150  of  13,469.\n",
      "  Batch 12,200  of  13,469.\n",
      "  Batch 12,250  of  13,469.\n",
      "  Batch 12,300  of  13,469.\n",
      "  Batch 12,350  of  13,469.\n",
      "  Batch 12,400  of  13,469.\n",
      "  Batch 12,450  of  13,469.\n",
      "  Batch 12,500  of  13,469.\n",
      "  Batch 12,550  of  13,469.\n",
      "  Batch 12,600  of  13,469.\n",
      "  Batch 12,650  of  13,469.\n",
      "  Batch 12,700  of  13,469.\n",
      "  Batch 12,750  of  13,469.\n",
      "  Batch 12,800  of  13,469.\n",
      "  Batch 12,850  of  13,469.\n",
      "  Batch 12,900  of  13,469.\n",
      "  Batch 12,950  of  13,469.\n",
      "  Batch 13,000  of  13,469.\n",
      "  Batch 13,050  of  13,469.\n",
      "  Batch 13,100  of  13,469.\n",
      "  Batch 13,150  of  13,469.\n",
      "  Batch 13,200  of  13,469.\n",
      "  Batch 13,250  of  13,469.\n",
      "  Batch 13,300  of  13,469.\n",
      "  Batch 13,350  of  13,469.\n",
      "  Batch 13,400  of  13,469.\n",
      "  Batch 13,450  of  13,469.\n",
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'time_formats'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-c03f5142d864>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m#evaluate model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mvalid_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Evaluation done for epoch {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-28-b915cd6618f4>\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m50\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[1;31m# Calculate elapsed time in minutes.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m             \u001b[0melapsed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mformat_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m             \u001b[1;31m# Report progress.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'  Batch {:>5,}  of  {:>5,}.'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvaldata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\develop\\Python\\Python38\\lib\\site-packages\\babel\\dates.py\u001b[0m in \u001b[0;36mformat_time\u001b[1;34m(time, format, tzinfo, locale)\u001b[0m\n\u001b[0;32m    806\u001b[0m     \u001b[0mlocale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLocale\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocale\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    807\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'full'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'long'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'medium'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'short'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 808\u001b[1;33m         \u001b[0mformat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_time_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlocale\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    809\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mparse_pattern\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocale\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\develop\\Python\\Python38\\lib\\site-packages\\babel\\dates.py\u001b[0m in \u001b[0;36mget_time_format\u001b[1;34m(format, locale)\u001b[0m\n\u001b[0;32m    424\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlocale\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mLocale\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlocale\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m     \"\"\"\n\u001b[1;32m--> 426\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mLocale\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocale\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime_formats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'time_formats'"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "\n",
    "# Empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "# for each epoch perform training and evaluation\n",
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "    #train model\n",
    "    train_loss, _ = trainBERT()\n",
    "    \n",
    "    #evaluate model\n",
    "    valid_loss, _ = evaluate()\n",
    "    \n",
    "    print('Evaluation done for epoch {}'.format(epoch + 1))\n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        print('Saving model...')\n",
    "        torch.save(model.state_dict(), 'bert_weights.pth') # Save model weight's (you can also save it in .bin format)\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict\n",
    "print('\\nTest Set...')\n",
    "\n",
    "test_preds = []\n",
    "\n",
    "print('Total batches:', len(testdata))\n",
    "\n",
    "for fold_index in range(0, 3):\n",
    "    \n",
    "    print('\\nFold Model', fold_index)\n",
    "    \n",
    "    # Load the fold model\n",
    "    path_model = 'bert_weights.pth'\n",
    "    model.load_state_dict(torch.load(path_model))\n",
    "\n",
    "    # Send the model to the GPU\n",
    "    model.to(device)\n",
    "\n",
    "    stacked_val_labels = []\n",
    "    \n",
    "    # Put the model in evaluation mode.\n",
    "    model.eval()\n",
    "\n",
    "    # Turn off the gradient calculations.\n",
    "    # This tells the model not to compute or store gradients.\n",
    "    # This step saves memory and speeds up validation.\n",
    "    torch.set_grad_enabled(False)\n",
    "\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_val_loss = 0\n",
    "\n",
    "    for j, test_batch in enumerate(testdata):\n",
    "\n",
    "        inference_status = 'Batch ' + str(j + 1)\n",
    "\n",
    "        print(inference_status, end='\\r')\n",
    "\n",
    "        b_input_ids = test_batch[0].to(device)\n",
    "        b_input_mask = test_batch[1].to(device)\n",
    "        b_token_type_ids = test_batch[2].to(device)\n",
    "        b_test_y = test_batch[3].to(device)\n",
    "\n",
    "\n",
    "        outputs = model(b_input_ids, \n",
    "                        attention_mask=b_input_mask,\n",
    "                        token_type_ids=b_token_type_ids)\n",
    "\n",
    "        # Get the preds\n",
    "        preds = outputs[0]\n",
    "\n",
    "        # Move preds to the CPU\n",
    "        val_preds = preds.detach().cpu().numpy()\n",
    "        \n",
    "        #true_labels.append(b_test_y.to('cpu').numpy().flatten())\n",
    "        \n",
    "        # Stack the predictions.\n",
    "        if j == 0:  # first batch\n",
    "            stacked_val_preds = val_preds\n",
    "            \n",
    "        else:\n",
    "            stacked_val_preds = np.vstack((stacked_val_preds, val_preds))\n",
    "            \n",
    "    test_preds.append(stacked_val_preds)\n",
    "    \n",
    "            \n",
    "print('\\nPrediction complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, item in enumerate(test_preds):\n",
    "    if i == 0:\n",
    "        preds = item\n",
    "    else:\n",
    "        # Sum the matrices\n",
    "        preds = item + preds\n",
    "\n",
    "# Average the predictions\n",
    "avg_preds = preds/(len(test_preds))\n",
    "\n",
    "#print(preds)\n",
    "#print()\n",
    "#print(avg_preds)\n",
    "\n",
    "# Take the argmax. \n",
    "# This returns the column index of the max value in each row.\n",
    "test_predictions = np.argmax(avg_preds, axis=1)\n",
    "\n",
    "# Take a look of the output\n",
    "print(type(test_predictions))\n",
    "print(len(test_predictions))\n",
    "print()\n",
    "print(test_predictions)\n",
    "\n",
    "true_y = []\n",
    "for j, test_batch in enumerate(testdata):\n",
    "    true_y.append(int(test_batch[3][0].numpy().flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualtion\n",
    "\n",
    "target_names = ['true_y', 'predicted_y']\n",
    "\n",
    "data = {'true_y': true_y,\n",
    "       'predicted_y': test_predictions}\n",
    "\n",
    "df_pred_BERT = pd.DataFrame(data, columns=['true_y','predicted_y'])\n",
    "\n",
    "confusion_matrix = pd.crosstab(df_pred_BERT['true_y'], df_pred_BERT['predicted_y'], rownames=['True'], colnames=['Predicted'])\n",
    "\n",
    "sns.heatmap(confusion_matrix, annot=True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0843aa2147bb7b68e1331c060614b1ebfeaba0f0db744f4b489daeb337a1f0b2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
