{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "#tokenizer setting \n",
    "NUM_WORD = 10000\n",
    "SENTENCE_LEN = 100\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "EMBED_DIM = 1000\n",
    "batch_size = 128\n",
    "import random\n",
    "random.seed(3344)\n",
    "np.random.seed(3344)\n",
    "torch.manual_seed(3344)\n",
    "torch.cuda.manual_seed(3344)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>terrible place to work for i just heard a stor...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>hours , minutes total time for an extremely s...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>my less than stellar review is for service . w...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>i m granting one star because there s no way t...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>the food here is mediocre at best . i went aft...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                                             review  split\n",
       "0       0  terrible place to work for i just heard a stor...  train\n",
       "1       0   hours , minutes total time for an extremely s...  train\n",
       "2       0  my less than stellar review is for service . w...  train\n",
       "3       0  i m granting one star because there s no way t...  train\n",
       "4       0  the food here is mediocre at best . i went aft...  train"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path ='reviews_with_splits_lite.csv'\n",
    "data = pd.read_csv(data_path,delimiter=',')\n",
    "#positive reviews 1 negative reviews 0\n",
    "data['rating']=data['rating'].apply(lambda x: 0 if x=='negative' else 1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data[data.split=='train']\n",
    "val_data = data[data.split=='val']\n",
    "test_data = data[data.split=='test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make tokenizer\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(data.review)\n",
    "#transform to numerical value\n",
    "train_seq = tokenizer.texts_to_sequences(train_data.review)\n",
    "val_seq = tokenizer.texts_to_sequences(val_data.review)\n",
    "test_seq = tokenizer.texts_to_sequences(test_data.review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39200\n"
     ]
    }
   ],
   "source": [
    "# print(train_seq[0])\n",
    "x_train = pad_sequences(train_seq,maxlen=SENTENCE_LEN)\n",
    "print(len(x_train))\n",
    "x_val = pad_sequences(val_seq,maxlen=SENTENCE_LEN)\n",
    "x_test = pad_sequences(test_seq,maxlen=SENTENCE_LEN)\n",
    "#make dataloader \n",
    "class text_Dataset(Dataset):\n",
    "    def __init__(self,data,label):\n",
    "        self.data = torch.tensor(data).to(torch.int64)\n",
    "        self.label = label\n",
    "    def __getitem__(self,index):\n",
    "        data = self.data[index]\n",
    "        label = self.label[index]\n",
    "        return data,label \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "#print(len(train_data['rating']),train_data['rating'][39200])\n",
    "train_dataset = text_Dataset(x_train,train_data.rating.to_numpy())\n",
    "#print(len(train_dataset),len(train_dataset[0]),train_dataset[19600])\n",
    "val_dataset = text_Dataset(x_val,val_data.rating.to_numpy())\n",
    "test_dataset = text_Dataset(x_test,test_data.rating.to_numpy())\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset,batch_size=batch_size,shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset,batch_size=batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build model\n",
    "#Text CNN\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim,output_dim,filter_sizes, num_filter=1,\n",
    "                  dropout=0.2, pad_idx=0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels=1, out_channels=num_filter,\n",
    "                      kernel_size=(fs, embedding_dim))\n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "        # in_channels：输入的channel，文字都是1\n",
    "        # out_channels：输出的channel维度\n",
    "        # fs：每次滑动窗口计算用到几个单词,相当于n-gram中的n\n",
    "        # for fs in filter_sizes用好几个卷积模型最后concate起来看效果。\n",
    "\n",
    "        self.fc = nn.Linear(len(filter_sizes) * num_filter, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.dropout(self.embedding(text))  # [batch size, sent len, emb dim]\n",
    "        embedded = embedded.unsqueeze(1)  # [batch size, 1, sent len, emb dim]\n",
    "        #print(embedded.shape)\n",
    "        # 升维是为了和nn.Conv2d的输入维度吻合，把channel列升维。\n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "        print(conved[0].shape,conved[1].shape,conved[2].shape)\n",
    "        # conved = [batch size, num_filter, sent len - filter_sizes+1]\n",
    "        # 有几个filter_sizes就有几个conved\n",
    "\n",
    "        pooled = [F.max_pool1d(conv,conv.shape[2]).squeeze(2) for conv in conved]  # [batch,num_filter]\n",
    "        print(pooled[0].shape,pooled[1].shape,pooled[2].shape)\n",
    "        x_cat=torch.cat(pooled, dim=1)\n",
    "        print(x_cat.shape)\n",
    "        cat = self.dropout(x_cat)\n",
    "        # cat = [batch size, num_filter * len(filter_sizes)]\n",
    "        # 把 len(filter_sizes)个卷积模型concate起来传到全连接层。\n",
    "        return self.fc(cat)\n",
    "        \n",
    "# model = CNN(SENTENCE_LEN,EMBED_DIM,output_dim=2,filter_sizes=[2,3,4]).to(device)\n",
    "# model.eval()\n",
    "class TCNN(nn.Module):\n",
    "    def __init__(self,embed_dim):\n",
    "        super(TCNN,self).__init__()\n",
    "        self.embed = nn.Embedding(NUM_WORD,embed_dim)\n",
    "        self.conv1 = nn.Conv2d(1,1,3)\n",
    "        self.conv2 = nn.Conv2d(1,1,3)\n",
    "        self.conv3 = nn.Conv2d(1,1,3)\n",
    "        self.fc = nn.Linear(93436,2)\n",
    "    def forward(self,x):\n",
    "        out = self.embed(x)\n",
    "        out = out.unsqueeze(1)\n",
    "        #print(out.shape)\n",
    "        out = F.relu(self.conv1(out))\n",
    "        #print(out.shape)\n",
    "        out = F.relu(self.conv2(out))\n",
    "        #print(out.shape)\n",
    "        out = F.relu(self.conv3(out))\n",
    "        #print(out.shape)\n",
    "        out = out.view(out.size()[0],-1)\n",
    "        #print(out.shape)\n",
    "        out = self.fc(out)\n",
    "        return out \n",
    "model = TCNN(EMBED_DIM).to(device)\n",
    "# train setting \n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/010] 68.92 sec(s) Train Acc: 0.726122 Loss: 0.004106 | Val Acc: 0.819881 loss: 0.003265\n",
      "[002/010] 71.74 sec(s) Train Acc: 0.935332 Loss: 0.001360 | Val Acc: 0.842262 loss: 0.003070\n",
      "[003/010] 73.48 sec(s) Train Acc: 0.987934 Loss: 0.000296 | Val Acc: 0.845119 loss: 0.004115\n",
      "[004/010] 73.10 sec(s) Train Acc: 0.996888 Loss: 0.000091 | Val Acc: 0.844048 loss: 0.005253\n",
      "[005/010] 73.47 sec(s) Train Acc: 0.998367 Loss: 0.000043 | Val Acc: 0.848452 loss: 0.005844\n",
      "[006/010] 73.47 sec(s) Train Acc: 0.998776 Loss: 0.000027 | Val Acc: 0.846310 loss: 0.006478\n",
      "[007/010] 73.86 sec(s) Train Acc: 0.999260 Loss: 0.000020 | Val Acc: 0.847619 loss: 0.006858\n",
      "[008/010] 74.25 sec(s) Train Acc: 0.999337 Loss: 0.000017 | Val Acc: 0.847262 loss: 0.007282\n",
      "[009/010] 74.46 sec(s) Train Acc: 0.999260 Loss: 0.000014 | Val Acc: 0.846310 loss: 0.007669\n",
      "[010/010] 76.56 sec(s) Train Acc: 0.999388 Loss: 0.000012 | Val Acc: 0.847619 loss: 0.007909\n"
     ]
    }
   ],
   "source": [
    "#training loop\n",
    "step =0\n",
    "import time\n",
    "for epoch in range(epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    model.train()\n",
    "    train_acc =0.0\n",
    "    val_acc =0.0\n",
    "    train_loss = 0.0\n",
    "    val_loss =0.0\n",
    "    for i,data in enumerate(train_dataloader):\n",
    "        step+=1\n",
    "        #print(step)\n",
    "        x = data[0].to(device)\n",
    "        y = data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        # print(out)\n",
    "        # print(y)\n",
    "        # break\n",
    "        loss = criterion(out,y.long())\n",
    "        loss.backward()\n",
    "        #losses.append(loss.item())\n",
    "        optimizer.step()\n",
    "        train_acc +=np.sum(np.argmax(out.cpu().data.numpy(),axis=1)== y.cpu().numpy())\n",
    "        train_loss +=loss.item()\n",
    "        #y_pred.append(torch.argmax(out,dim=1))\n",
    "    #validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i,data in enumerate(val_dataloader):\n",
    "            valx ,valy = data[0].to(device),data[1].to(device)\n",
    "            val_pred = model(valx)\n",
    "            batch_loss = criterion(val_pred,valy.long())\n",
    "            val_acc +=np.sum(np.argmax(val_pred.cpu().data.numpy(),axis=1)== valy.cpu().numpy())\n",
    "            val_loss +=batch_loss.item()\n",
    "\n",
    "        print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f | Val Acc: %3.6f loss: %3.6f' % \\\n",
    "            (epoch + 1, epochs, time.time()-epoch_start_time, \\\n",
    "             train_acc/train_dataset.__len__(), train_loss/train_dataset.__len__(), val_acc/val_dataset.__len__(), val_loss/val_dataset.__len__()))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc: 0.8505952380952381\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "model.eval()\n",
    "prediction = []\n",
    "with torch.no_grad():\n",
    "    for i,data in enumerate(test_dataloader):\n",
    "        testx = data[0].to(device)\n",
    "        test_pred = model(testx)\n",
    "        test_label = np.argmax(test_pred.cpu().data.numpy(),axis=1)\n",
    "        for y in test_label:\n",
    "            prediction.append(y)\n",
    "\n",
    "acc = accuracy_score(test_data.rating.to_numpy(),prediction)\n",
    "print('test acc:',acc)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0843aa2147bb7b68e1331c060614b1ebfeaba0f0db744f4b489daeb337a1f0b2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
