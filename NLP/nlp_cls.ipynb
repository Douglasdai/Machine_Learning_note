{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP 数据处理+ 分类\n",
    "### 采用了fake news dataset <https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset>\n",
    "### 使用pyTorch 框架或TF 主要学习数据处理，怎么处理文本到向量的转变\n",
    "### 部分代码参考 <https://www.kaggle.com/rushinaik/mission-torch-1#Train-Test-Splitting>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "#TfidVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "import random\n",
    "import scipy\n",
    "from torch import optim\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "seed_val = 1234\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
       "      <td>WASHINGTON (Reuters) - The head of a conservat...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. military to accept transgender recruits o...</td>\n",
       "      <td>WASHINGTON (Reuters) - Transgender people will...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>\n",
       "      <td>WASHINGTON (Reuters) - The special counsel inv...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FBI Russia probe helped by Australian diplomat...</td>\n",
       "      <td>WASHINGTON (Reuters) - Trump campaign adviser ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 30, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump wants Postal Service to charge 'much mor...</td>\n",
       "      <td>SEATTLE/WASHINGTON (Reuters) - President Donal...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  As U.S. budget fight looms, Republicans flip t...   \n",
       "1  U.S. military to accept transgender recruits o...   \n",
       "2  Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
       "3  FBI Russia probe helped by Australian diplomat...   \n",
       "4  Trump wants Postal Service to charge 'much mor...   \n",
       "\n",
       "                                                text       subject  \\\n",
       "0  WASHINGTON (Reuters) - The head of a conservat...  politicsNews   \n",
       "1  WASHINGTON (Reuters) - Transgender people will...  politicsNews   \n",
       "2  WASHINGTON (Reuters) - The special counsel inv...  politicsNews   \n",
       "3  WASHINGTON (Reuters) - Trump campaign adviser ...  politicsNews   \n",
       "4  SEATTLE/WASHINGTON (Reuters) - President Donal...  politicsNews   \n",
       "\n",
       "                 date  label  \n",
       "0  December 31, 2017       1  \n",
       "1  December 29, 2017       1  \n",
       "2  December 31, 2017       1  \n",
       "3  December 30, 2017       1  \n",
       "4  December 29, 2017       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#overview the data\n",
    "df1 = pd.read_csv('D:/ML_data_sql/news/True.csv')\n",
    "df2 = pd.read_csv('D:/ML_data_sql/news/Fake.csv')\n",
    "df1['label'] = 1\n",
    "df2['label'] = 0\n",
    "df = pd.concat([df1, df2], axis=0)\n",
    "\n",
    "del df1 \n",
    "del df2\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataset: (44898, 5)\n",
      "\n",
      "Sum of nulls:\n",
      "title      0\n",
      "text       0\n",
      "subject    0\n",
      "date       0\n",
      "label      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape of the dataset: {df.shape}')\n",
    "print(f'\\nSum of nulls:\\n{df.isna().sum()}')\n",
    "#发现没有空的 空的可能要补上之类的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 44898 entries, 0 to 23480\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   title    44898 non-null  object\n",
      " 1   text     44898 non-null  object\n",
      " 2   subject  44898 non-null  object\n",
      " 3   date     44898 non-null  object\n",
      " 4   label    44898 non-null  int64 \n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 2.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([31428, 10000])\n"
     ]
    }
   ],
   "source": [
    "#preprocessing \n",
    "def normalise_text(text):\n",
    "    #re  to normalise\n",
    "    text = text.str.lower()\n",
    "    text = text.str.replace(r\"\\#\",'')\n",
    "    text = text.str.replace(r'http\\S+','URL')\n",
    "    text = text.str.replace(r'@','')\n",
    "    text = text.str.replace(r\"[^A-Za-z0-9()!?\\'\\`\\\"]\",' ')\n",
    "    text = text.str.replace(\"\\s{2,}\",\" \")\n",
    "\n",
    "    return text\n",
    "\n",
    "df['text'] = df['title'] +\" \"+df['text']\n",
    "df['text'] = normalise_text(df['text'])\n",
    "del df['title']\n",
    "del df['subject']\n",
    "del df['date']\n",
    "#split train and test\n",
    "#7:3\n",
    "#word -> vector  simple\n",
    "X_train,X_test,y_train,y_test = train_test_split(df['text'],df['label'],test_size=0.3)\n",
    "#create vector \n",
    "vectorize = TfidfVectorizer(ngram_range=(1,1),max_features=10000)\n",
    "\n",
    "X_train = vectorize.fit_transform(X_train)\n",
    "X_test = vectorize.fit_transform(X_test)\n",
    " \n",
    "#稠密矩阵\n",
    "X_train = torch.tensor(X_train.todense()).float() \n",
    "X_test = torch.Tensor(X_test.todense()).float()\n",
    "y_train = torch.tensor(y_train.values)\n",
    "y_test = torch.tensor(y_test.values)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# model = nn.Sequential(\n",
    "#                 nn.Linear(X_train.shape[1],128 ),\n",
    "#                 nn.ReLU(),\n",
    "#                 nn.Dropout(0.1),\n",
    "#                 nn.Linear(128, df['label'].nunique()),\n",
    "#                 nn.LogSoftmax(dim=1)\n",
    "# )\n",
    "# # defining the loss \n",
    "# criterion = nn.NLLLoss()\n",
    "\n",
    "# # Forward pass, get our logits\n",
    "# logps = model(X_train)\n",
    "\n",
    "# # Calculate the loss with the logits and the labels\n",
    "# loss = criterion(logps, y_train)\n",
    "\n",
    "\n",
    "# loss.backward()\n",
    "\n",
    "# # Optimizers require the parameters to optimize and a learning rate\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.002)\n",
    "\n",
    "# train_losses = []\n",
    "# test_losses = []\n",
    "# test_accuracies = []\n",
    "\n",
    "# epochs = 100\n",
    "# for e in range(epochs):\n",
    "#     optimizer.zero_grad()\n",
    "\n",
    "#     output = model.forward(X_train)\n",
    "#     loss = criterion(output, y_train)\n",
    "#     loss.backward()\n",
    "#     train_loss = loss.item()\n",
    "#     train_losses.append(train_loss)\n",
    "    \n",
    "#     optimizer.step()\n",
    "    \n",
    "    \n",
    "#     # Turn off gradients for validation, saves memory and computations\n",
    "#     with torch.no_grad():\n",
    "#         model.eval()\n",
    "#         log_ps = model(X_test)\n",
    "#         test_loss = criterion(log_ps, y_test)\n",
    "#         test_losses.append(test_loss)\n",
    "\n",
    "#         ps = torch.exp(log_ps)\n",
    "#         top_p, top_class = ps.topk(1, dim=1)\n",
    "#         equals = top_class == y_test.view(*top_class.shape)\n",
    "#         test_accuracy = torch.mean(equals.float())\n",
    "#         test_accuracies.append(test_accuracy)\n",
    "\n",
    "#     model.train()\n",
    "#     if (e+1)%10==0:\n",
    "#         print(f\"Epoch: {e+1}/{epochs}.. \",\n",
    "#               f\"Training Loss: {train_loss:.3f}.. \",\n",
    "#               f\"Test Loss: {test_loss:.3f}.. \",\n",
    "#               f\"Test Accuracy: {test_accuracy:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.61 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#train acc = 1  test acc = 0.62\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "class NLPDataset(Dataset):\n",
    "    def __init__(self, x, y=None):\n",
    "        self.x = x\n",
    "        # label is required to be a LongTensor\n",
    "        self.y = y\n",
    "        if y is not None:\n",
    "            self.y = torch.LongTensor(y)\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, index):\n",
    "        X = self.x[index]\n",
    "        if self.y is not None:\n",
    "            Y = self.y[index]\n",
    "            return X, Y\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "batch_size= 1024\n",
    "train_set = NLPDataset(X_train,y_train)\n",
    "test_set = NLPDataset(X_test,y_test)\n",
    "train_loader = DataLoader(train_set,batch_size = batch_size,shuffle=True)\n",
    "test_loader = DataLoader(test_set,batch_size=batch_size,shuffle=False)\n",
    "#model \n",
    "import torch.nn.functional as F\n",
    "class LR(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LR,self).__init__()\n",
    "        #tensor 50* 50*1\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(10000, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(128,2)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        out = self.fc(x)\n",
    "        #out = F.softmax(out,dim=1)\n",
    "        return out \n",
    "class ANN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(ANN,self).__init__() # Inhertiting\n",
    "        \n",
    "        self.linear1 = nn.Linear(10000,2000) # IN 5008 OUT 2000\n",
    "        self.relu1 = nn.ReLU() # Actfunc 1\n",
    "        \n",
    "        self.linear2 = nn.Linear(2000,500) # IN 2000 OUT 500\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.linear3 = nn.Linear(500,100) # IN 500 OUT 100\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        self.linear4 = nn.Linear(100,20) # IN 100 OUT 20\n",
    "        self.relu4 = nn.ReLU()\n",
    "        \n",
    "        self.linear5 = nn.Linear(20,2) # IN 20 OUT 2 | OUTPUT \n",
    "        \n",
    "    \n",
    "    def forward(self,x):\n",
    "        \n",
    "        out = self.linear1(x) # Input Layer \n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        out = self.linear2(out) # Hidden Layer 1 \n",
    "        out = self.relu2(out)\n",
    "        \n",
    "        out = self.linear3(out) # Hidden Layer 2 \n",
    "        out = self.relu3(out)\n",
    "        \n",
    "        out = self.linear4(out) # Hidden Layer 3 \n",
    "        out = self.relu4(out)\n",
    "\n",
    "        \n",
    "        out = self.linear5(out) # Output Layer\n",
    "        \n",
    "        return out\n",
    "# class TextCNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(TextCNN,self).__init__()\n",
    "#         self.cnn = nn.Sequential(\n",
    "#             nn.Conv1d(in_channels=1,out_channels=1,kernel_size=3,padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv1d(in_channels=1,out_channels=1,kernel_size=3,padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv1d(in_channels=1,out_channels=1,kernel_size=3,padding=1),\n",
    "#             nn.ReLU(),\n",
    "#         )\n",
    "#         self.fc = nn.Linear(10000,2)\n",
    "#     def forward(self,x):\n",
    "#         x = self.cnn(x)\n",
    "#         x = x.view(x.size(0),-1)\n",
    "#         out = self.fc(x)\n",
    "#         return out\n",
    "    \n",
    "#settings \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'\n",
    "model = ANN().to(device)\n",
    "cirection = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr =0.002)\n",
    "import time\n",
    "epochs = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of continuous-multioutput and binary targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-0eee36ae8028>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;31m#print(len(mm),len(mm2))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[0mtrain_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmm2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m     \u001b[0mtest_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mm2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f | Val Acc: %3.6f loss: %3.6f' % \\\n",
      "\u001b[1;32mC:\\develop\\Python\\Python38\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\develop\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m     \u001b[1;31m# Compute accuracy for each possible representation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'multilabel'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\develop\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0m\u001b[0;32m     93\u001b[0m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of continuous-multioutput and binary targets"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "for epoch in range(epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    train_acc =0.0\n",
    "    val_acc =0.0\n",
    "    train_loss = 0.0\n",
    "    val_loss =0.0\n",
    "    y_pred_total = []\n",
    "    y_true= []\n",
    "    y_val_total = []\n",
    "    y_val_true= []\n",
    "    model.train()\n",
    "    for i,data in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        x,y = data[0].to(device),data[1].to(device)\n",
    "        #print(y.shape)\n",
    "        y_pred = model(x)\n",
    "        #print(y_pred.shape)\n",
    "        loss = cirection(y_pred,y.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        y_pred_total.append(np.argmax(y_pred.cpu().detach().numpy(),axis=1))\n",
    "        # print(len(y_pred_total))\n",
    "        y_true.append(y.cpu().detach().numpy())\n",
    "        # print(len(y_true))\n",
    "        # break\n",
    "        #train_acc +=np.sum(np.argmax(y_pred.cpu().data.numpy(),axis=1)== y.cpu().numpy())\n",
    "        train_loss +=loss.item()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i,data in enumerate(test_loader):\n",
    "            valx ,valy = data[0].to(device),data[1].to(device)\n",
    "            val_pred = model(valx)\n",
    "            y_val_total.append(val_pred.cpu().detach().numpy())\n",
    "            y_val_true.append(valy.cpu().detach().numpy())\n",
    "            batch_loss = cirection(val_pred,valy.long())\n",
    "            #val_acc +=np.sum(np.argmax(val_pred.cpu().data.numpy(),axis=1)== valy.cpu().numpy())\n",
    "            val_loss +=batch_loss.item()\n",
    "    #print(y_pred_total.size,y_true.shape)\n",
    "    mm = [j for i in y_pred_total for j in i]\n",
    "    mm2 = [j for i in y_true for j in i]\n",
    "    #print(len(merge_list),len(merge_list2))\n",
    "    #train_acc = accuracy_score(merge_list,merge_list2)\n",
    "    #print(train_acc)\n",
    "    #print(len(y_pred_total[0]),len(y_true[0]))\n",
    "    #break\n",
    "    m1 = [j for i in y_val_total for j in i]\n",
    "    m2 = [j for i in y_val_true for j in i]\n",
    "    #print(len(mm),len(mm2))\n",
    "    train_acc = accuracy_score(mm,mm2)\n",
    "    test_acc = accuracy_score(m1,m2)\n",
    "    if epoch %10==0: \n",
    "        print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f | Val Acc: %3.6f loss: %3.6f' % \\\n",
    "            (epoch + 1, epochs, time.time()-epoch_start_time, \\\n",
    "                train_acc, train_loss/train_set.__len__(), val_acc, val_loss/test_set.__len__()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0843aa2147bb7b68e1331c060614b1ebfeaba0f0db744f4b489daeb337a1f0b2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
