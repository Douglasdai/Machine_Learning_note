{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP 数据处理+ 分类\n",
    "### 采用了fake news dataset <https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset>\n",
    "### 使用pyTorch 框架或TF 主要学习数据处理，怎么处理文本到向量的转变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "#TfidVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from tensorflow.python.keras.models import Model\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "#from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "#from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "from torch import optim\n",
    "%matplotlib inline\n",
    "seed_val = 1234\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "NUM_WORD = 10000\n",
    "EMBED_DIM = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
       "      <td>WASHINGTON (Reuters) - The head of a conservat...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. military to accept transgender recruits o...</td>\n",
       "      <td>WASHINGTON (Reuters) - Transgender people will...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>\n",
       "      <td>WASHINGTON (Reuters) - The special counsel inv...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FBI Russia probe helped by Australian diplomat...</td>\n",
       "      <td>WASHINGTON (Reuters) - Trump campaign adviser ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 30, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump wants Postal Service to charge 'much mor...</td>\n",
       "      <td>SEATTLE/WASHINGTON (Reuters) - President Donal...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  As U.S. budget fight looms, Republicans flip t...   \n",
       "1  U.S. military to accept transgender recruits o...   \n",
       "2  Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
       "3  FBI Russia probe helped by Australian diplomat...   \n",
       "4  Trump wants Postal Service to charge 'much mor...   \n",
       "\n",
       "                                                text       subject  \\\n",
       "0  WASHINGTON (Reuters) - The head of a conservat...  politicsNews   \n",
       "1  WASHINGTON (Reuters) - Transgender people will...  politicsNews   \n",
       "2  WASHINGTON (Reuters) - The special counsel inv...  politicsNews   \n",
       "3  WASHINGTON (Reuters) - Trump campaign adviser ...  politicsNews   \n",
       "4  SEATTLE/WASHINGTON (Reuters) - President Donal...  politicsNews   \n",
       "\n",
       "                 date  label  \n",
       "0  December 31, 2017       1  \n",
       "1  December 29, 2017       1  \n",
       "2  December 31, 2017       1  \n",
       "3  December 30, 2017       1  \n",
       "4  December 29, 2017       1  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#overview the data\n",
    "df1 = pd.read_csv('D:/ML_data_sql/news/True.csv')\n",
    "df2 = pd.read_csv('D:/ML_data_sql/news/Fake.csv')\n",
    "df1['label'] = 1\n",
    "df2['label'] = 0\n",
    "df = pd.concat([df1, df2], axis=0)\n",
    "del df1 \n",
    "del df2\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataset: (44898, 2)\n",
      "\n",
      "Sum of nulls:\n",
      "text     0\n",
      "label    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f'Shape of the dataset: {df.shape}')\n",
    "print(f'\\nSum of nulls:\\n{df.isna().sum()}')\n",
    "#发现没有空的 空的可能要补上之类的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "def normalise_text(text):\n",
    "    #re  to normalise\n",
    "    text = text.str.lower()\n",
    "    text = text.str.replace(r\"\\#\",'')\n",
    "    text = text.str.replace(r'http\\S+','URL')\n",
    "    text = text.str.replace(r'@','')\n",
    "    text = text.str.replace(r\"[^A-Za-z0-9()!?\\'\\`\\\"]\",' ')\n",
    "    text = text.str.replace(\"\\s{2,}\",\" \")\n",
    "\n",
    "    return text\n",
    "\n",
    "df['text'] = df['title'] +\" \"+df['text']\n",
    "df['text'] = normalise_text(df['text'])\n",
    "del df['title']\n",
    "del df['subject']\n",
    "del df['date']\n",
    "#split train and test\n",
    "#7:3\n",
    "#word -> vector  simple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader,Dataset\n",
    "X_train,X_test,y_train,y_test = train_test_split(df['text'],df['label'],test_size=0.2)\n",
    "NUM_WORDS = 10000\n",
    "SENTENCE_LENGTH = 100\n",
    "#make features\n",
    "tokenizer = Tokenizer(num_words=NUM_WORDS) \n",
    "tokenizer.fit_on_texts(df['text'])\n",
    "train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "x_train = pad_sequences(train_seq,maxlen=SENTENCE_LENGTH)\n",
    "x_test = pad_sequences(test_seq,maxlen=SENTENCE_LENGTH)\n",
    "class text_Dataset(Dataset):\n",
    "    def __init__(self,data,label):\n",
    "        self.data = torch.tensor(data).to(torch.int64)\n",
    "        self.label = label\n",
    "    def __getitem__(self,index):\n",
    "        data = self.data[index]\n",
    "        label = self.label[index]\n",
    "        return data,label \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "batch_size=256\n",
    "train_dataset = text_Dataset(x_train,y_train.to_numpy())\n",
    "test_dataset = text_Dataset(x_test,y_test.to_numpy())\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset,batch_size=batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class TCNN(nn.Module):\n",
    "    def __init__(self,embed_dim):\n",
    "        super(TCNN,self).__init__()\n",
    "        self.embed = nn.Embedding(NUM_WORD,embed_dim)\n",
    "        self.conv1 = nn.Conv2d(1,1,3)\n",
    "        self.conv2 = nn.Conv2d(1,1,3)\n",
    "        self.conv3 = nn.Conv2d(1,1,3)\n",
    "        self.fc = nn.Linear(93436,2)\n",
    "    def forward(self,x):\n",
    "        out = self.embed(x)\n",
    "        out = out.unsqueeze(1)\n",
    "        #print(out.shape)\n",
    "        out = F.relu(self.conv1(out))\n",
    "        #print(out.shape)\n",
    "        out = F.relu(self.conv2(out))\n",
    "        #print(out.shape)\n",
    "        out = F.relu(self.conv3(out))\n",
    "        #print(out.shape)\n",
    "        out = out.view(out.size()[0],-1)\n",
    "        #print(out.shape)\n",
    "        out = self.fc(out)\n",
    "        return out \n",
    "model = TCNN(EMBED_DIM).to(device)\n",
    "# train setting \n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/010] 69.89 sec(s) Train Acc: 0.983462 Loss: 0.000211 | Val Acc: 0.974833 loss: 0.000288\n",
      "[002/010] 66.92 sec(s) Train Acc: 0.998914 Loss: 0.000032 | Val Acc: 0.975947 loss: 0.000309\n",
      "[003/010] 64.50 sec(s) Train Acc: 0.999722 Loss: 0.000011 | Val Acc: 0.976726 loss: 0.000323\n",
      "[004/010] 64.49 sec(s) Train Acc: 0.999722 Loss: 0.000009 | Val Acc: 0.975501 loss: 0.000349\n",
      "[005/010] 64.49 sec(s) Train Acc: 0.983323 Loss: 0.000277 | Val Acc: 0.975724 loss: 0.000275\n",
      "[006/010] 64.50 sec(s) Train Acc: 0.999805 Loss: 0.000013 | Val Acc: 0.976169 loss: 0.000290\n",
      "[007/010] 64.49 sec(s) Train Acc: 0.999944 Loss: 0.000004 | Val Acc: 0.977506 loss: 0.000307\n",
      "[008/010] 64.50 sec(s) Train Acc: 0.999889 Loss: 0.000004 | Val Acc: 0.977060 loss: 0.000312\n",
      "[009/010] 64.49 sec(s) Train Acc: 0.999916 Loss: 0.000003 | Val Acc: 0.977394 loss: 0.000320\n",
      "[010/010] 64.51 sec(s) Train Acc: 0.999972 Loss: 0.000002 | Val Acc: 0.976726 loss: 0.000326\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#training loop\n",
    "step =0\n",
    "import time\n",
    "for epoch in range(epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    model.train()\n",
    "    train_acc =0.0\n",
    "    val_acc =0.0\n",
    "    train_loss = 0.0\n",
    "    val_loss =0.0\n",
    "    for i,data in enumerate(train_dataloader):\n",
    "        step+=1\n",
    "        #print(step)\n",
    "        x = data[0].to(device)\n",
    "        y = data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        # print(out)\n",
    "        # print(y)\n",
    "        # break\n",
    "        loss = criterion(out,y.long())\n",
    "        loss.backward()\n",
    "        #losses.append(loss.item())\n",
    "        optimizer.step()\n",
    "        train_acc +=np.sum(np.argmax(out.cpu().data.numpy(),axis=1)== y.cpu().numpy())\n",
    "        train_loss +=loss.item()\n",
    "        #y_pred.append(torch.argmax(out,dim=1))\n",
    "    #validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i,data in enumerate(test_dataloader):\n",
    "            valx ,valy = data[0].to(device),data[1].to(device)\n",
    "            val_pred = model(valx)\n",
    "            batch_loss = criterion(val_pred,valy.long())\n",
    "            val_acc +=np.sum(np.argmax(val_pred.cpu().data.numpy(),axis=1)== valy.cpu().numpy())\n",
    "            val_loss +=batch_loss.item()\n",
    "\n",
    "        print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f | Val Acc: %3.6f loss: %3.6f' % \\\n",
    "            (epoch + 1, epochs, time.time()-epoch_start_time, \\\n",
    "             train_acc/train_dataset.__len__(), train_loss/train_dataset.__len__(), val_acc/test_dataset.__len__(), val_loss/test_dataset.__len__()))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#create vector \n",
    "# vectorize = TfidfVectorizer(ngram_range=(1,1),max_features=10000)\n",
    "\n",
    "# X_train = vectorize.fit_transform(X_train)\n",
    "# X_test = vectorize.fit_transform(X_test)\n",
    " \n",
    "# #稠密矩阵\n",
    "# X_train = torch.tensor(X_train.todense()).float() \n",
    "# X_test = torch.Tensor(X_test.todense()).float()\n",
    "# y_train = torch.tensor(y_train.values)\n",
    "# y_test = torch.tensor(y_test.values)\n",
    "# print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# model = nn.Sequential(\n",
    "#                 nn.Linear(X_train.shape[1],128 ),\n",
    "#                 nn.ReLU(),\n",
    "#                 nn.Dropout(0.1),\n",
    "#                 nn.Linear(128, df['label'].nunique()),\n",
    "#                 nn.LogSoftmax(dim=1)\n",
    "# )\n",
    "# # defining the loss \n",
    "# criterion = nn.NLLLoss()\n",
    "\n",
    "# # Forward pass, get our logits\n",
    "# logps = model(X_train)\n",
    "\n",
    "# # Calculate the loss with the logits and the labels\n",
    "# loss = criterion(logps, y_train)\n",
    "\n",
    "\n",
    "# loss.backward()\n",
    "\n",
    "# # Optimizers require the parameters to optimize and a learning rate\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.002)\n",
    "\n",
    "# train_losses = []\n",
    "# test_losses = []\n",
    "# test_accuracies = []\n",
    "\n",
    "# epochs = 100\n",
    "# for e in range(epochs):\n",
    "#     optimizer.zero_grad()\n",
    "\n",
    "#     output = model.forward(X_train)\n",
    "#     loss = criterion(output, y_train)\n",
    "#     loss.backward()\n",
    "#     train_loss = loss.item()\n",
    "#     train_losses.append(train_loss)\n",
    "    \n",
    "#     optimizer.step()\n",
    "    \n",
    "    \n",
    "#     # Turn off gradients for validation, saves memory and computations\n",
    "#     with torch.no_grad():\n",
    "#         model.eval()\n",
    "#         log_ps = model(X_test)\n",
    "#         test_loss = criterion(log_ps, y_test)\n",
    "#         test_losses.append(test_loss)\n",
    "\n",
    "#         ps = torch.exp(log_ps)\n",
    "#         top_p, top_class = ps.topk(1, dim=1)\n",
    "#         equals = top_class == y_test.view(*top_class.shape)\n",
    "#         test_accuracy = torch.mean(equals.float())\n",
    "#         test_accuracies.append(test_accuracy)\n",
    "\n",
    "#     model.train()\n",
    "#     if (e+1)%10==0:\n",
    "#         print(f\"Epoch: {e+1}/{epochs}.. \",\n",
    "#               f\"Training Loss: {train_loss:.3f}.. \",\n",
    "#               f\"Test Loss: {test_loss:.3f}.. \",\n",
    "#               f\"Test Accuracy: {test_accuracy:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# #train acc = 1  test acc = 0.62\n",
    "# from torch.utils.data import DataLoader,Dataset\n",
    "# class NLPDataset(Dataset):\n",
    "#     def __init__(self, x, y=None):\n",
    "#         self.x = x\n",
    "#         # label is required to be a LongTensor\n",
    "#         self.y = y\n",
    "#         if y is not None:\n",
    "#             self.y = torch.LongTensor(y)\n",
    "#     def __len__(self):\n",
    "#         return len(self.x)\n",
    "#     def __getitem__(self, index):\n",
    "#         X = self.x[index]\n",
    "#         if self.y is not None:\n",
    "#             Y = self.y[index]\n",
    "#             return X, Y\n",
    "#         else:\n",
    "#             return X\n",
    "\n",
    "# batch_size= 1024\n",
    "# train_set = NLPDataset(X_train,y_train)\n",
    "# test_set = NLPDataset(X_test,y_test)\n",
    "# train_loader = DataLoader(train_set,batch_size = batch_size,shuffle=True)\n",
    "# test_loader = DataLoader(test_set,batch_size=batch_size,shuffle=False)\n",
    "# #model \n",
    "# import torch.nn.functional as F\n",
    "# class LR(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(LR,self).__init__()\n",
    "#         #tensor 50* 50*1\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Linear(10000, 1024),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(1024, 512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(512, 128), \n",
    "#             nn.ReLU(), \n",
    "#             nn.Linear(128,2)\n",
    "#         )\n",
    "#     def forward(self,x):\n",
    "#         out = self.fc(x)\n",
    "#         #out = F.softmax(out,dim=1)\n",
    "#         return out \n",
    "# class ANN(nn.Module):\n",
    "    \n",
    "#     def __init__(self):\n",
    "        \n",
    "#         super(ANN,self).__init__() # Inhertiting\n",
    "        \n",
    "#         self.linear1 = nn.Linear(10000,2000) # IN 5008 OUT 2000\n",
    "#         self.relu1 = nn.ReLU() # Actfunc 1\n",
    "        \n",
    "#         self.linear2 = nn.Linear(2000,500) # IN 2000 OUT 500\n",
    "#         self.relu2 = nn.ReLU()\n",
    "        \n",
    "#         self.linear3 = nn.Linear(500,100) # IN 500 OUT 100\n",
    "#         self.relu3 = nn.ReLU()\n",
    "        \n",
    "#         self.linear4 = nn.Linear(100,20) # IN 100 OUT 20\n",
    "#         self.relu4 = nn.ReLU()\n",
    "        \n",
    "#         self.linear5 = nn.Linear(20,2) # IN 20 OUT 2 | OUTPUT \n",
    "        \n",
    "    \n",
    "#     def forward(self,x):\n",
    "        \n",
    "#         out = self.linear1(x) # Input Layer \n",
    "#         out = self.relu1(out)\n",
    "        \n",
    "#         out = self.linear2(out) # Hidden Layer 1 \n",
    "#         out = self.relu2(out)\n",
    "        \n",
    "#         out = self.linear3(out) # Hidden Layer 2 \n",
    "#         out = self.relu3(out)\n",
    "        \n",
    "#         out = self.linear4(out) # Hidden Layer 3 \n",
    "#         out = self.relu4(out)\n",
    "\n",
    "        \n",
    "#         out = self.linear5(out) # Output Layer\n",
    "        \n",
    "#         return out\n",
    "# # class TextCNN(nn.Module):\n",
    "# #     def __init__(self):\n",
    "# #         super(TextCNN,self).__init__()\n",
    "# #         self.cnn = nn.Sequential(\n",
    "# #             nn.Conv1d(in_channels=1,out_channels=1,kernel_size=3,padding=1),\n",
    "# #             nn.ReLU(),\n",
    "# #             nn.Conv1d(in_channels=1,out_channels=1,kernel_size=3,padding=1),\n",
    "# #             nn.ReLU(),\n",
    "# #             nn.Conv1d(in_channels=1,out_channels=1,kernel_size=3,padding=1),\n",
    "# #             nn.ReLU(),\n",
    "# #         )\n",
    "# #         self.fc = nn.Linear(10000,2)\n",
    "# #     def forward(self,x):\n",
    "# #         x = self.cnn(x)\n",
    "# #         x = x.view(x.size(0),-1)\n",
    "# #         out = self.fc(x)\n",
    "# #         return out\n",
    "    \n",
    "# #settings \n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# # device = 'cpu'\n",
    "# model = ANN().to(device)\n",
    "# cirection = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(),lr =0.002)\n",
    "# import time\n",
    "# epochs = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "# for epoch in range(epochs):\n",
    "#     epoch_start_time = time.time()\n",
    "#     train_acc =0.0\n",
    "#     val_acc =0.0\n",
    "#     train_loss = 0.0\n",
    "#     val_loss =0.0\n",
    "#     y_pred_total = []\n",
    "#     y_true= []\n",
    "#     y_val_total = []\n",
    "#     y_val_true= []\n",
    "#     model.train()\n",
    "#     for i,data in enumerate(train_loader):\n",
    "#         optimizer.zero_grad()\n",
    "#         x,y = data[0].to(device),data[1].to(device)\n",
    "#         #print(y.shape)\n",
    "#         y_pred = model(x)\n",
    "#         #print(y_pred.shape)\n",
    "#         loss = cirection(y_pred,y.long())\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         y_pred_total.append(np.argmax(y_pred.cpu().detach().numpy(),axis=1))\n",
    "#         # print(len(y_pred_total))\n",
    "#         y_true.append(y.cpu().detach().numpy())\n",
    "#         # print(len(y_true))\n",
    "#         # break\n",
    "#         #train_acc +=np.sum(np.argmax(y_pred.cpu().data.numpy(),axis=1)== y.cpu().numpy())\n",
    "#         train_loss +=loss.item()\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         for i,data in enumerate(test_loader):\n",
    "#             valx ,valy = data[0].to(device),data[1].to(device)\n",
    "#             val_pred = model(valx)\n",
    "#             y_val_total.append(val_pred.cpu().detach().numpy())\n",
    "#             y_val_true.append(valy.cpu().detach().numpy())\n",
    "#             batch_loss = cirection(val_pred,valy.long())\n",
    "#             #val_acc +=np.sum(np.argmax(val_pred.cpu().data.numpy(),axis=1)== valy.cpu().numpy())\n",
    "#             val_loss +=batch_loss.item()\n",
    "#     #print(y_pred_total.size,y_true.shape)\n",
    "#     mm = [j for i in y_pred_total for j in i]\n",
    "#     mm2 = [j for i in y_true for j in i]\n",
    "#     #print(len(merge_list),len(merge_list2))\n",
    "#     #train_acc = accuracy_score(merge_list,merge_list2)\n",
    "#     #print(train_acc)\n",
    "#     #print(len(y_pred_total[0]),len(y_true[0]))\n",
    "#     #break\n",
    "#     m1 = [j for i in y_val_total for j in i]\n",
    "#     m2 = [j for i in y_val_true for j in i]\n",
    "#     #print(len(mm),len(mm2))\n",
    "#     train_acc = accuracy_score(mm,mm2)\n",
    "#     test_acc = accuracy_score(m1,m2)\n",
    "#     if epoch %10==0: \n",
    "#         print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f | Val Acc: %3.6f loss: %3.6f' % \\\n",
    "#             (epoch + 1, epochs, time.time()-epoch_start_time, \\\n",
    "#                 train_acc, train_loss/train_set.__len__(), val_acc, val_loss/test_set.__len__()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0843aa2147bb7b68e1331c060614b1ebfeaba0f0db744f4b489daeb337a1f0b2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
