{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from math import sqrt\n",
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(real, predict):\n",
    "    res = 0\n",
    "    count = 0\n",
    "    for i in range(len(real)):\n",
    "        if real[i] != 0:\n",
    "            res += abs((predict[i]-real[i])/real[i])\n",
    "            count += 1\n",
    "    return res/count\n",
    "\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    \"\"\"\n",
    "\tFrame a time series as a supervised learning dataset.\n",
    "\tArguments:\n",
    "\t\tdata: Sequence of observations as a list or NumPy array.\n",
    "\t\tn_in: Number of lag observations as input (X).\n",
    "\t\tn_out: Number of observations as output (y).\n",
    "\t\tdropnan: Boolean whether or not to drop rows with NaN values.\n",
    "\tReturns:\n",
    "\t\tPandas DataFrame of series framed for supervised learning.\n",
    "\t\"\"\"\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j + 1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j + 1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j + 1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1106, 4)\n",
      "   var1(t-1)  var2(t-1)  var3(t-1)  var4(t-1)   var4(t)\n",
      "1   0.000012   0.538462   0.000000   0.183087  0.343635\n",
      "2   0.000012   0.487179   0.000000   0.343635  0.485551\n",
      "3   0.000012   0.653846   0.000000   0.485551  0.530059\n",
      "4   0.000006   0.525641   0.000000   0.530059  0.542535\n",
      "5   0.000002   0.730769   0.012283   0.542535  0.535841\n",
      "(1106, 1, 4) (1106,) (0, 1, 4) (0,)\n",
      "1 4\n",
      "Epoch 1/50\n",
      "16/16 - 0s - loss: nan\n",
      "Epoch 2/50\n",
      "16/16 - 0s - loss: nan\n",
      "Epoch 3/50\n",
      "16/16 - 0s - loss: nan\n",
      "Epoch 4/50\n",
      "16/16 - 0s - loss: nan\n",
      "Epoch 5/50\n",
      "16/16 - 0s - loss: nan\n",
      "Epoch 6/50\n",
      "16/16 - 0s - loss: nan\n",
      "Epoch 7/50\n",
      "16/16 - 0s - loss: nan\n",
      "Epoch 8/50\n",
      "16/16 - 0s - loss: nan\n",
      "Epoch 9/50\n",
      "16/16 - 0s - loss: nan\n",
      "Epoch 10/50\n",
      "16/16 - 0s - loss: nan\n",
      "Epoch 11/50\n",
      "16/16 - 0s - loss: nan\n",
      "Epoch 12/50\n",
      "16/16 - 0s - loss: nan\n",
      "Epoch 13/50\n",
      "16/16 - 0s - loss: nan\n",
      "Epoch 14/50\n",
      "16/16 - 0s - loss: nan\n",
      "Epoch 15/50\n",
      "16/16 - 0s - loss: nan\n",
      "Epoch 16/50\n",
      "16/16 - 0s - loss: nan\n",
      "Epoch 17/50\n",
      "16/16 - 0s - loss: nan\n",
      "Epoch 18/50\n",
      "16/16 - 0s - loss: nan\n",
      "Epoch 19/50\n",
      "16/16 - 0s - loss: nan\n",
      "Epoch 20/50\n",
      "16/16 - 0s - loss: nan\n",
      "Epoch 21/50\n",
      "16/16 - 0s - loss: nan\n",
      "Epoch 22/50\n",
      "16/16 - 0s - loss: nan\n",
      "Epoch 23/50\n",
      "16/16 - 0s - loss: nan\n",
      "Epoch 24/50\n",
      "16/16 - 0s - loss: nan\n",
      "Epoch 25/50\n",
      "16/16 - 0s - loss: nan\n",
      "Epoch 26/50\n",
      "16/16 - 0s - loss: nan\n",
      "Epoch 27/50\n",
      "16/16 - 0s - loss: nan\n",
      "Epoch 28/50\n",
      "16/16 - 0s - loss: nan\n",
      "Epoch 29/50\n",
      "16/16 - 0s - loss: nan\n",
      "Epoch 30/50\n",
      "16/16 - 0s - loss: nan\n",
      "Epoch 31/50\n",
      "16/16 - 0s - loss: nan\n",
      "Epoch 32/50\n",
      "16/16 - 0s - loss: nan\n",
      "Epoch 33/50\n",
      "16/16 - 0s - loss: nan\n",
      "Epoch 34/50\n",
      "16/16 - 0s - loss: nan\n",
      "Epoch 35/50\n",
      "16/16 - 0s - loss: nan\n",
      "Epoch 36/50\n",
      "16/16 - 0s - loss: nan\n",
      "Epoch 37/50\n",
      "16/16 - 0s - loss: nan\n",
      "Epoch 38/50\n",
      "16/16 - 0s - loss: nan\n",
      "Epoch 39/50\n",
      "16/16 - 0s - loss: nan\n",
      "Epoch 40/50\n",
      "16/16 - 0s - loss: nan\n",
      "Epoch 41/50\n",
      "16/16 - 0s - loss: nan\n",
      "Epoch 42/50\n",
      "16/16 - 0s - loss: nan\n",
      "Epoch 43/50\n",
      "16/16 - 0s - loss: nan\n",
      "Epoch 44/50\n",
      "16/16 - 0s - loss: nan\n",
      "Epoch 45/50\n",
      "16/16 - 0s - loss: nan\n",
      "Epoch 46/50\n",
      "16/16 - 0s - loss: nan\n",
      "Epoch 47/50\n",
      "16/16 - 0s - loss: nan\n",
      "Epoch 48/50\n",
      "16/16 - 0s - loss: nan\n",
      "Epoch 49/50\n",
      "16/16 - 0s - loss: nan\n",
      "Epoch 50/50\n",
      "16/16 - 0s - loss: nan\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-84-25284f78b1d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;31m# plot history\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[0mpyplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m \u001b[0mpyplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[0mpyplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[0mpyplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'val_loss'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load dataset\n",
    "'''\n",
    "header = 0: 排除第一行数据； header = -1: 包含第一行数据；\n",
    "index_col=0：排除第一列数据；index_col=-1：包含第一列数据；\n",
    "'''\n",
    "dataset = read_csv('ele_pre.csv', header=0,index_col=0)\n",
    "dataset.columns = ['high_tem', 'low_tem', 'avg_tem','com_him','rain','power']\n",
    "values = dataset.values\n",
    "values = values.astype('float32')\n",
    "# dataset.head\n",
    "values = np.delete(values,[0,1],axis=1)\n",
    "print(values.shape)\n",
    "# normalize features\n",
    "# scaler = MinMaxScaler().fit(values)\n",
    "# scaled = scaler.fit_transform(values)\n",
    "# frame as supervised learning\n",
    "# split into train and test sets\n",
    "values = MinMaxScaler().fit_transform(values)  \n",
    "reframed = series_to_supervised(values, 1, 1)\n",
    "# drop columns we don't want to predict\n",
    "#reframed.drop(reframed.columns[[7,8,9,10,11]], axis=1, inplace=True)\n",
    "reframed.drop(reframed.columns[[4,5,6]], axis=1, inplace=True)\n",
    "print(reframed.head())\n",
    "\n",
    "n_train_hours = 365 * 24 \n",
    "train = values[:n_train_hours, :]\n",
    "test = values[n_train_hours:, :]\n",
    "# # split into input and outputs\n",
    "train_X, train_y = train[:, :], train[:, -1]\n",
    "test_X, test_y = test[:, :], test[:, -1]\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "print(train_X.shape[1],train_X.shape[2])\n",
    "# design network\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "# fit network\n",
    "history = model.fit(train_X, train_y, epochs=50, batch_size=72, validation_data=(test_X, test_y), verbose=2,\n",
    "                    shuffle=False)\n",
    "# plot history\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model,self).__init__()\n",
    "        self.fc1 = nn.Linear(6,100)\n",
    "        self.fc2 = nn.Linear(100,256)\n",
    "        self.fc3 = nn.Linear(256,512)\n",
    "        self.fc4 = nn.Linear(512,128)\n",
    "        self.fc5 = nn.Linear(128,1)\n",
    "        \n",
    "    \n",
    "    def forward(self,x):\n",
    "        out = self.fc1(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc4(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc5(out)\n",
    "        return out\n",
    "\n",
    "model = Model()\n",
    "#setting\n",
    "learing_rate = 0.05\n",
    "epochs = 500\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learing_rate)\n",
    "# optimizer = torch.optim.Adagrad(torch.nn.parameter(),lr=0.01)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = torch.from_numpy(train_X)\n",
    "train_y =torch.from_numpy(train_y)\n",
    "test_X = torch.from_numpy(test_X)\n",
    "test_y= torch.from_numpy(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    y_pred = model(train_X)\n",
    "    y2_pred = y_pred.squeeze()\n",
    "    loss = criterion(y2_pred,train_y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_dlr_pred = model(test_X)\n",
    "        y3_pred = y_dlr_pred.squeeze()\n",
    "        testloss = criterion(y3_pred,test_y)\n",
    "    if(epoch+1)%50==0:\n",
    "        print(\"loss: {:2f} test loss: {:2f}\".format(loss.item(),testloss.item()))\n",
    "\n",
    "\n",
    "# mape = mean_absolute_percentage_error(y3_pred, test_y)\n",
    "# print('Test mape: %.3f' % mape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0843aa2147bb7b68e1331c060614b1ebfeaba0f0db744f4b489daeb337a1f0b2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
