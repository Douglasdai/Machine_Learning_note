{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.6.0'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "from csv import reader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_data(filepath,n_train_data=1096):\n",
    "#     dataset = []\n",
    "#     with open(filepath,'r') as f:\n",
    "#         csv_reader =reader(f,delimiter = ',')\n",
    "#         for row in csv_reader:\n",
    "#             row[1:] = list(map(float,row[1:]))\n",
    "#             #row[0] = int(row[0])\n",
    "#             dataset.append(row[1:])\n",
    "#         dataset = np.array(dataset)\n",
    "#         #归一化\n",
    "#         # mms = MinMaxScaler()\n",
    "#         # for i in range(dataset.shape[0]-1):\n",
    "#         #     dataset[:,i] = mms.fit_transform(dataset[:,i].reshape(-1,1)).flatten()\n",
    "#         #print(dataset[0])\n",
    "        \n",
    "#         #split_data\n",
    "#         train_data = dataset[0:n_train_data]\n",
    "#         val_data = dataset[n_train_data:]  \n",
    "\n",
    "#         return train_data,val_data\n",
    "\n",
    "# train_data,test_data = load_data('./ele_pre.csv')   \n",
    "# print(train_data[0])\n",
    "# print(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对14年12月和15年 前10天的电力负载进行预测\n",
    "DAYS_FOR_TRAIN = 365\n",
    "class LSTM_Regression(nn.Module):\n",
    "    \"\"\"\n",
    "        使用LSTM进行回归\n",
    "        \n",
    "        参数：\n",
    "        - input_size: feature size\n",
    "        - hidden_size: number of hidden units\n",
    "        - output_size: number of output\n",
    "        - num_layers: layers of LSTM to stack\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size=1, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    def forward(self, _x):\n",
    "        x, _ = self.lstm(_x)  # _x is input, size (seq_len, batch, input_size)\n",
    "        s, b, h = x.shape  # x is output, size (seq_len, batch, hidden_size)\n",
    "        x = x.view(s*b, h)\n",
    "        x = self.fc(x)\n",
    "        x = x.view(s, b, -1)  # 把形状改回来\n",
    "        return x\n",
    "\n",
    "class BPNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BPNet,self).__init__()\n",
    "        self.lr1 = nn.Linear(5,1000)        \n",
    "        self.lr2 = nn.Linear(1000,2000)        \n",
    "        self.lr3 = nn.Linear(2000,512)        \n",
    "        self.lr4 = nn.Linear(512,100)        \n",
    "        self.lr5 = nn.Linear(100,10)        \n",
    "        self.lr6 = nn.Linear(10,1)        \n",
    "    def forward(self,x):\n",
    "        out = self.lr1(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.lr2(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.lr3(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.lr4(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.lr5(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.lr6(out)\n",
    "        return out\n",
    "\n",
    "model = BPNet()\n",
    "#model = LSTM_Regression(DAYS_FOR_TRAIN, 8, output_size=1, num_layers=2)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-80-edb90817bfc9>:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_x = torch.tensor(train_x,dtype=torch.float32)\n",
      "<ipython-input-80-edb90817bfc9>:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_y = torch.tensor(train_y,dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# def create_dataset(data, days_for_train=1):\n",
    "#     \"\"\"\n",
    "#         根据给定的序列data，生成数据集\n",
    "        \n",
    "#         数据集分为输入和输出，每一个输入的长度为days_for_train，每一个输出的长度为1。\n",
    "#         也就是说用days_for_train天的数据，对应下一天的数据。\n",
    "#         若给定序列的长度为d，将输出长度为(d-days_for_train+1)个输入/输出对\n",
    "#     \"\"\"\n",
    "#     dataset_x, dataset_y= [], []\n",
    "#     for i in range(len(data)-days_for_train):\n",
    "#         _x = data[i:(i+days_for_train)]\n",
    "#         dataset_x.append(_x)\n",
    "#         dataset_y.append(data[i+days_for_train])\n",
    "#     return (np.array(dataset_x), np.array(dataset_y))\n",
    "# print(ele[0],type(ele[0]))\n",
    "# dataset_x, dataset_y = create_dataset(ele, DAYS_FOR_TRAIN)\n",
    "# #dataset_y 为下一年的数据\n",
    "# # print(dataset_x[0][0])\n",
    "# # print(dataset_y[0])\n",
    "# train_size = int(len(dataset_x) * 0.7)\n",
    "# train_x = dataset_x[:train_size]\n",
    "# train_y = dataset_y[:train_size]\n",
    "# # 将数据改变形状，RNN 读入的数据维度是 (seq_size, batch_size, feature_size)\n",
    "# train_x = train_x.reshape(-1, 1, DAYS_FOR_TRAIN)\n",
    "# train_y = train_y.reshape(-1, 1, 1)\n",
    "# # 转为pytorch的tensor对象\n",
    "# train_x = torch.from_numpy(train_x)\n",
    "# train_y = torch.from_numpy(train_y)\n",
    "# train_data = torch.from_numpy(train_data)\n",
    "\n",
    "pre_data = np.genfromtxt('ele_pre.txt',delimiter='\\t',dtype=float)\n",
    "# print(pre_data[0])\n",
    "train_x = pre_data[:,1:6]\n",
    "train_x = torch.from_numpy(train_x)\n",
    "train_x = torch.tensor(train_x,dtype=torch.float32)\n",
    "train_y = pre_data[:,6:7]\n",
    "# train_y[0]\n",
    "train_y = torch.from_numpy(train_y)\n",
    "train_y = torch.tensor(train_y,dtype=torch.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100, Loss:nan\n",
      "Epoch: 200, Loss:nan\n",
      "Epoch: 300, Loss:nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-94-7cf52f384ff3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Epoch: {}, Loss:{:.5f}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\develop\\Python\\Python38\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\develop\\Python\\Python38\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    109\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m                 \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(1000):  \n",
    "    optimizer.zero_grad()\n",
    "    #print(train_x.shape)                 \n",
    "    out = model(train_x)\n",
    "    loss = loss_function(out,train_y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (i+1) % 100 == 0:\n",
    "        print('Epoch: {}, Loss:{:.5f}'.format(i+1, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "model = model.eval() # 转换成测试模式\n",
    "# 注意这里用的是全集 模型的输出长度会比原数据少DAYS_FOR_TRAIN 填充使长度相等再作图\n",
    "dataset_x = dataset_x.reshape(-1, 1, DAYS_FOR_TRAIN)  # (seq_size, batch_size, feature_size)\n",
    "dataset_x = torch.from_numpy(dataset_x)\n",
    "pred_test = model(dataset_x) # 全量训练集的模型输出 (seq_size, batch_size, output_size)\n",
    "pred_test = pred_test.view(-1).data.numpy()\n",
    "pred_test = np.concatenate((np.zeros(DAYS_FOR_TRAIN), pred_test))  # 填充0 使长度相同\n",
    "assert len(pred_test) == len(ele)\n",
    "plt.plot(pred_test, 'r', label='prediction')\n",
    "plt.plot(ele, 'b', label='real')\n",
    "plt.plot((train_size, train_size), (0, 1), 'g--')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "\n",
    "# csvFile = open(\"./ele_pre.csv\",'w',newline='',encoding='utf-8')\n",
    "\n",
    "# writer = csv.writer(csvFile)\n",
    "\n",
    "# csvRow = []\n",
    "# f = open(\"ele_pre.txt\",'r',encoding='GB2312')\n",
    "\n",
    "# for line in f:\n",
    "#     csvRow = line.split()\n",
    "\n",
    "#     writer.writerow(csvRow)\n",
    "# f.close()\n",
    "# csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0843aa2147bb7b68e1331c060614b1ebfeaba0f0db744f4b489daeb337a1f0b2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
