{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from bs4 import BeautifulSoup\n",
    "import torchvision\n",
    "from torchvision import transforms,datasets,models\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#引入预训练模型\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "import matplotlib.patches as patches\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#在 xml 中读取bbox\n",
    "def generate_box(obj):\n",
    "    xmin = int(obj.find('xmin').text)\n",
    "    ymin = int(obj.find('ymin').text)\n",
    "    xmax = int(obj.find('xmax').text)\n",
    "    ymax = int(obj.find('ymax').text)\n",
    "    return [xmin,ymin,xmax,ymax]\n",
    "\n",
    "#在xml 读取label 二分类，为person\n",
    "def generate_person_label(obj):\n",
    "    #person 返回1 不是返回0\n",
    "    return obj.find('name').text =='person'\n",
    "\n",
    "def generate_target(image_id,file):\n",
    "    with open(file,'r',encoding='utf-8') as f :\n",
    "        data = f.read()\n",
    "        soup  = BeautifulSoup(data,'lxml-xml')\n",
    "        objs = soup.find_all('object')\n",
    "        nums_objs = len(objs)\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for i in objs:\n",
    "            boxes.append(generate_box(i))\n",
    "            labels.append(generate_person_label(i))\n",
    "        #make data\n",
    "        boxes = torch.as_tensor(boxes,dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels,dtype=torch.int64)\n",
    "        img_id = torch.tensor([image_id])\n",
    "        target = {}\n",
    "        target['boxes'] = boxes\n",
    "        target['labels'] = labels\n",
    "        target['image_id'] = img_id\n",
    "        return target\n",
    "\n",
    "\n",
    "#因为不对等 所以要把图片名字和xml文件名字对应起来\n",
    "# labels = [img[:-4]+'.xml' for img in imgs]\n",
    "labels = list(sorted(os.listdir('D:\\\\ML_data_sql\\\\human_detect\\\\annotations\\\\')))\n",
    "imgs = [label[:-4]+'.jpg' for label in labels]\n",
    "#使用一部分数据（0.2倍）\n",
    "use_labels = labels[:int(len(labels)*0.2)]\n",
    "use_imgs = imgs[:int(len(imgs)*0.2)]\n",
    "test_imgs = imgs[int(len(imgs)*0.2):int(len(imgs)*0.21)]\n",
    "test_labels = labels[int(len(labels)*0.2):int(len(labels)*0.21)]\n",
    "# print(len(labels))\n",
    "# print(len(imgs))\n",
    "# print(imgs[0],labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make dataset\n",
    "class PersonDataset(object):\n",
    "    def __init__(self,transforms,imgs,labels):\n",
    "        self.transforms = transforms\n",
    "        self.imgs = imgs\n",
    "        self.labels = labels\n",
    "        self.lens = len(imgs)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        #load images and masks\n",
    "        img = Image.open('D:\\\\ML_data_sql\\\\human_detect\\\\JPEGImages\\\\'+self.imgs[idx])\n",
    "        #label = generate_target(idx,'D:\\\\ML_data_sql\\\\human_detect\\\\annotations\\\\'+self.labels[idx])\n",
    "        target = generate_target(idx,'D:\\\\ML_data_sql\\\\human_detect\\\\annotations\\\\'+self.labels[idx])\n",
    "        #apply transforms\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "        return img,target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "        transforms.ToTensor(), \n",
    "    ])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "dataset = PersonDataset(data_transform,use_imgs,use_labels)\n",
    "test_dataset =PersonDataset(data_transform,test_imgs,test_labels)\n",
    "#因为dataset 有点大 爆显存 所以使用少量来训练\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "traindata_loader = torch.utils.data.DataLoader(dataset, batch_size=4,collate_fn=collate_fn)\n",
    "testdata_loader = torch.utils.data.DataLoader(test_dataset, batch_size=4,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the model\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model\n",
    "#load the model\n",
    "model = get_model_instance_segmentation(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "#训练前 数据查看\n",
    "#model.to(device)\n",
    "for imgs,annotations in traindata_loader:\n",
    "    imgs = list(img.to(device) for img in imgs)\n",
    "    annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "    print(annotations)\n",
    "    #包含了bbox 和label 格式正确\n",
    "    #model_outputs = model(imgs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#begin training\n",
    "from tqdm.notebook import tqdm\n",
    "import time \n",
    "epochs = 1\n",
    "\n",
    "model.to(device)\n",
    "#超参数设置，SGD 优化器，采用动量法衰减学习率，每次更新学习率为原来的0.9倍\n",
    "params =  [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.005,momentum=0.9,weight_decay=0.0005)\n",
    "len_dataloaer = len(traindata_loader)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    i=0\n",
    "    epoch_loss = 0\n",
    "    with tqdm(traindata_loader) as iterator:\n",
    "        for imgs,annotations in iterator:\n",
    "            i+=1\n",
    "            imgs = list(img.to(device) for img in imgs)\n",
    "            annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "            #print(len(imgs),len(annotations)\n",
    "            #计算loss\n",
    "            loss_dict = model([imgs[0]],[annotations[0]])\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += losses\n",
    "    #每10个epoch 输出一次loss\n",
    "    if i%10==0:\n",
    "        torch.save(model.state_dict(),'./model/model_'+str(epoch+1)+'.pth')\n",
    "        print('epoch:',epoch+1,'i:',i,'loss:',epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "# plt_imgs = []\n",
    "# plt_pred = []\n",
    "# model.load_state_dict(torch.load('./model/model_begin.pt'))\n",
    "# model.eval()\n",
    "# model.to(device)\n",
    "# for imgs, annotations in testdata_loader:\n",
    "#         imgs = list(img.to(device) for img in imgs)\n",
    "#         annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "#         plt_imgs.append(imgs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #限于显存原因，只能够采用少量数据来训练，并且只能只用每批batch中的第一个来训练\n",
    "# model.load_state_dict(torch.load('./model/model_begin.pt'))\n",
    "# input_img,labels = test_dataset[0]\n",
    "# print(test_dataset.lens)\n",
    "# input_img = input_img.unsqueeze(0)\n",
    "# print(input_img.shape)\n",
    "# pred = model(input_img.to(device))[0]\n",
    "# print(pred)\n",
    "# pred_box = pred['boxes'].detach().cpu().numpy()\n",
    "# print(pred_box)\n",
    "# real = labels['boxes'].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_image(img_tensor,pred):\n",
    "#     fig,ax = plt.subplots(1)\n",
    "#     img = img_tensor.cpu().data\n",
    "#     # Display the image\n",
    "#     ax.imshow(img.permute(1, 2, 0))\n",
    "#     xmin,ymin,xmax,ymax = pred[0][1],pred[0][1],pred[0][2],pred[0][3]\n",
    "#     #xmin, ymin, xmax, ymax = [427.7730, 534.9896, 638.9386, 720.0000]\n",
    "#     # Create a Rectangle patch\n",
    "#     rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=2,edgecolor='white',facecolor='none') \n",
    "#     # Add the patch to the Axes\n",
    "#     ax.add_patch(rect)\n",
    "#     plt.show()\n",
    "# plot_image(input_img.squeeze(0),pred_box)\n",
    "# plot_image(input_img.squeeze(0),real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate\n",
    "#look up to predict_img.py\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('deepl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "22378730f7bb3d512c488123b229e59741e8adca80ef7eaa27c2c48a74342c5f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
