{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import cv2\n",
    "import torch.nn as nn \n",
    "import pandas as pd\n",
    "import os\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training 時做 data augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomHorizontalFlip(), # 隨機將圖片水平翻轉\n",
    "    transforms.RandomRotation(15), # 隨機旋轉圖片\n",
    "    transforms.ToTensor(), # 將圖片轉成 Tensor，並把數值 normalize 到 [0,1] (data normalization)\n",
    "])\n",
    "# testing 時不需做 data augmentation\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),                                    \n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "class ImgDataset(Dataset):\n",
    "    def __init__(self, x, y=None, transform=None):\n",
    "        self.x = x\n",
    "        # label is required to be a LongTensor\n",
    "        self.y = y\n",
    "        if y is not None:\n",
    "            self.y = torch.LongTensor(y.numpy())\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, index):\n",
    "        X = self.x[index]\n",
    "        if self.transform is not None:\n",
    "            X = self.transform(X)\n",
    "        if self.y is not None:\n",
    "            Y = self.y[index]\n",
    "            return X, Y\n",
    "        else:\n",
    "            return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24946, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = np.load(\"training_data_128.npy\", allow_pickle = True)\n",
    "training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.Tensor([i[0] for i in training_data]).view(-1, 128, 128)\n",
    "# Scaling the features\n",
    "X = X/255.0\n",
    "# Getting the target\n",
    "y = torch.Tensor([i[1] for i in training_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24946, 2])\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_label = []\n",
    "real = torch.argmax(y)\n",
    "for i in range(len(y)):\n",
    "    real = torch.argmax(y[i])\n",
    "    y_label.append(real.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24946\n",
      "12476\n"
     ]
    }
   ],
   "source": [
    "print(len(y_label))\n",
    "count = 0\n",
    "for i in range(len(y_label)):\n",
    "    if y_label[i] ==0:\n",
    "        count+=1\n",
    "#功有 12476 猫 24946- 12476 只狗\n",
    "print(count)\n",
    "y_label = torch.Tensor(y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = int(len(X)*0.3)\n",
    "train_x = X[:-val]\n",
    "train_y = y_label[:-val]\n",
    "val_x = X[-val:]\n",
    "val_y = y_label[-val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "train_set = ImgDataset(train_x,train_y,train_transform)\n",
    "test_set = ImgDataset(val_x,val_y,test_transform)\n",
    "train_loader = DataLoader(train_set,batch_size = batch_size,shuffle=True)\n",
    "test_loader = DataLoader(test_set,batch_size=batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        # torch.nn.MaxPool2d(kernel_size, stride, padding)\n",
    "        # input 維度 [1, 128, 128]\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, 1, 1),  # [64, 128, 128]\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),      # [64, 64, 64]\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, 1, 1), # [128, 64, 64]\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),      # [128, 32, 32]\n",
    "\n",
    "            nn.Conv2d(128, 256, 3, 1, 1), # [256, 32, 32]\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),      # [256, 16, 16]\n",
    "\n",
    "            nn.Conv2d(256, 512, 3, 1, 1), # [512, 16, 16]\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),       # [512, 8, 8]\n",
    "            \n",
    "            nn.Conv2d(512, 512, 3, 1, 1), # [512, 8, 8]\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),       # [512, 4, 4]\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512*4*4, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(128,2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.cnn(x)\n",
    "        out = out.view(out.size()[0], -1)\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'\n",
    "# print(device)\n",
    "model = Classifier().to(device)\n",
    "cirection = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/080] 72.86 sec(s) Train Acc: 0.724618 Loss: 0.008468 | Val Acc: 0.755980 loss: 0.007630\n",
      "[002/080] 72.00 sec(s) Train Acc: 0.792075 Loss: 0.006937 | Val Acc: 0.763731 loss: 0.007292\n",
      "[003/080] 71.56 sec(s) Train Acc: 0.840634 Loss: 0.005609 | Val Acc: 0.858613 loss: 0.005121\n",
      "[004/080] 72.03 sec(s) Train Acc: 0.872301 Loss: 0.004684 | Val Acc: 0.784177 loss: 0.007915\n",
      "[005/080] 71.45 sec(s) Train Acc: 0.892802 Loss: 0.003952 | Val Acc: 0.891888 loss: 0.004112\n",
      "[006/080] 72.53 sec(s) Train Acc: 0.909179 Loss: 0.003407 | Val Acc: 0.918883 loss: 0.003059\n",
      "[007/080] 74.17 sec(s) Train Acc: 0.920060 Loss: 0.003055 | Val Acc: 0.915943 loss: 0.003222\n",
      "[008/080] 74.17 sec(s) Train Acc: 0.926072 Loss: 0.002798 | Val Acc: 0.929574 loss: 0.002639\n",
      "[009/080] 74.13 sec(s) Train Acc: 0.932314 Loss: 0.002534 | Val Acc: 0.907390 loss: 0.003560\n",
      "[010/080] 74.10 sec(s) Train Acc: 0.939415 Loss: 0.002311 | Val Acc: 0.932514 loss: 0.002798\n",
      "[011/080] 74.11 sec(s) Train Acc: 0.944282 Loss: 0.002135 | Val Acc: 0.918215 loss: 0.003214\n",
      "[012/080] 74.13 sec(s) Train Acc: 0.949035 Loss: 0.001986 | Val Acc: 0.934652 loss: 0.002747\n",
      "[013/080] 74.12 sec(s) Train Acc: 0.949493 Loss: 0.001958 | Val Acc: 0.924362 loss: 0.002868\n",
      "[014/080] 74.02 sec(s) Train Acc: 0.956537 Loss: 0.001716 | Val Acc: 0.935454 loss: 0.002589\n",
      "[015/080] 74.00 sec(s) Train Acc: 0.957682 Loss: 0.001642 | Val Acc: 0.942670 loss: 0.002421\n",
      "[016/080] 74.01 sec(s) Train Acc: 0.959457 Loss: 0.001585 | Val Acc: 0.936389 loss: 0.002521\n",
      "[017/080] 74.26 sec(s) Train Acc: 0.963580 Loss: 0.001464 | Val Acc: 0.942002 loss: 0.002441\n",
      "[018/080] 73.99 sec(s) Train Acc: 0.961977 Loss: 0.001433 | Val Acc: 0.939596 loss: 0.002501\n",
      "[019/080] 74.05 sec(s) Train Acc: 0.968562 Loss: 0.001227 | Val Acc: 0.949218 loss: 0.002740\n",
      "[020/080] 74.13 sec(s) Train Acc: 0.970795 Loss: 0.001221 | Val Acc: 0.933850 loss: 0.003008\n",
      "[021/080] 73.92 sec(s) Train Acc: 0.972284 Loss: 0.001104 | Val Acc: 0.942403 loss: 0.002362\n",
      "[022/080] 74.08 sec(s) Train Acc: 0.972227 Loss: 0.001082 | Val Acc: 0.948550 loss: 0.002748\n",
      "[023/080] 74.98 sec(s) Train Acc: 0.974403 Loss: 0.000972 | Val Acc: 0.941601 loss: 0.003306\n",
      "[024/080] 73.95 sec(s) Train Acc: 0.976751 Loss: 0.000940 | Val Acc: 0.949085 loss: 0.002425\n",
      "[025/080] 73.94 sec(s) Train Acc: 0.978755 Loss: 0.000883 | Val Acc: 0.948283 loss: 0.002875\n",
      "[026/080] 73.91 sec(s) Train Acc: 0.980645 Loss: 0.000841 | Val Acc: 0.947882 loss: 0.002566\n",
      "[027/080] 73.95 sec(s) Train Acc: 0.980988 Loss: 0.000788 | Val Acc: 0.950154 loss: 0.002404\n",
      "[028/080] 73.89 sec(s) Train Acc: 0.980530 Loss: 0.000808 | Val Acc: 0.949753 loss: 0.002429\n",
      "[029/080] 73.93 sec(s) Train Acc: 0.980988 Loss: 0.000779 | Val Acc: 0.946946 loss: 0.003143\n",
      "[030/080] 72.46 sec(s) Train Acc: 0.981446 Loss: 0.000728 | Val Acc: 0.927035 loss: 0.004528\n",
      "[031/080] 72.33 sec(s) Train Acc: 0.983966 Loss: 0.000654 | Val Acc: 0.942937 loss: 0.003056\n",
      "[032/080] 73.08 sec(s) Train Acc: 0.985913 Loss: 0.000589 | Val Acc: 0.948817 loss: 0.003001\n",
      "[033/080] 73.69 sec(s) Train Acc: 0.984596 Loss: 0.000607 | Val Acc: 0.938260 loss: 0.004695\n",
      "[034/080] 72.19 sec(s) Train Acc: 0.986772 Loss: 0.000548 | Val Acc: 0.946813 loss: 0.003791\n",
      "[035/080] 72.18 sec(s) Train Acc: 0.988318 Loss: 0.000502 | Val Acc: 0.949619 loss: 0.002975\n",
      "[036/080] 72.14 sec(s) Train Acc: 0.988948 Loss: 0.000482 | Val Acc: 0.955098 loss: 0.002788\n",
      "[037/080] 72.17 sec(s) Train Acc: 0.987975 Loss: 0.000516 | Val Acc: 0.944407 loss: 0.002933\n",
      "[038/080] 72.18 sec(s) Train Acc: 0.989177 Loss: 0.000432 | Val Acc: 0.952158 loss: 0.002890\n",
      "[039/080] 72.21 sec(s) Train Acc: 0.989063 Loss: 0.000487 | Val Acc: 0.945343 loss: 0.003160\n",
      "[040/080] 72.16 sec(s) Train Acc: 0.989063 Loss: 0.000465 | Val Acc: 0.950287 loss: 0.002788\n",
      "[041/080] 72.18 sec(s) Train Acc: 0.989177 Loss: 0.000490 | Val Acc: 0.954697 loss: 0.002277\n",
      "[042/080] 72.13 sec(s) Train Acc: 0.992613 Loss: 0.000328 | Val Acc: 0.950955 loss: 0.003193\n",
      "[043/080] 72.19 sec(s) Train Acc: 0.990036 Loss: 0.000407 | Val Acc: 0.953495 loss: 0.003033\n",
      "[044/080] 72.20 sec(s) Train Acc: 0.991754 Loss: 0.000373 | Val Acc: 0.929173 loss: 0.005015\n",
      "[045/080] 72.51 sec(s) Train Acc: 0.990265 Loss: 0.000449 | Val Acc: 0.947347 loss: 0.003110\n",
      "[046/080] 72.33 sec(s) Train Acc: 0.992441 Loss: 0.000386 | Val Acc: 0.953227 loss: 0.003044\n",
      "[047/080] 72.33 sec(s) Train Acc: 0.992498 Loss: 0.000324 | Val Acc: 0.950020 loss: 0.003614\n",
      "[048/080] 72.34 sec(s) Train Acc: 0.993186 Loss: 0.000297 | Val Acc: 0.924629 loss: 0.006675\n",
      "[049/080] 72.32 sec(s) Train Acc: 0.991983 Loss: 0.000357 | Val Acc: 0.950421 loss: 0.003536\n",
      "[050/080] 72.33 sec(s) Train Acc: 0.990494 Loss: 0.000431 | Val Acc: 0.951891 loss: 0.002673\n",
      "[051/080] 72.33 sec(s) Train Acc: 0.991869 Loss: 0.000336 | Val Acc: 0.954965 loss: 0.003324\n",
      "[052/080] 72.32 sec(s) Train Acc: 0.992785 Loss: 0.000303 | Val Acc: 0.955365 loss: 0.002979\n",
      "[053/080] 72.54 sec(s) Train Acc: 0.993930 Loss: 0.000285 | Val Acc: 0.951624 loss: 0.003127\n",
      "[054/080] 72.31 sec(s) Train Acc: 0.993415 Loss: 0.000297 | Val Acc: 0.955766 loss: 0.003311\n",
      "[055/080] 72.30 sec(s) Train Acc: 0.993243 Loss: 0.000316 | Val Acc: 0.956835 loss: 0.002736\n",
      "[056/080] 72.33 sec(s) Train Acc: 0.992441 Loss: 0.000349 | Val Acc: 0.898704 loss: 0.009855\n",
      "[057/080] 72.34 sec(s) Train Acc: 0.993357 Loss: 0.000295 | Val Acc: 0.955633 loss: 0.003031\n",
      "[058/080] 72.31 sec(s) Train Acc: 0.994331 Loss: 0.000247 | Val Acc: 0.956435 loss: 0.003102\n",
      "[059/080] 72.32 sec(s) Train Acc: 0.995419 Loss: 0.000210 | Val Acc: 0.952158 loss: 0.003794\n",
      "[060/080] 72.31 sec(s) Train Acc: 0.993300 Loss: 0.000271 | Val Acc: 0.947615 loss: 0.004986\n",
      "[061/080] 72.35 sec(s) Train Acc: 0.994159 Loss: 0.000257 | Val Acc: 0.944006 loss: 0.004101\n",
      "[062/080] 72.29 sec(s) Train Acc: 0.994732 Loss: 0.000237 | Val Acc: 0.952826 loss: 0.003666\n",
      "[063/080] 72.29 sec(s) Train Acc: 0.994216 Loss: 0.000255 | Val Acc: 0.953227 loss: 0.005238\n",
      "[064/080] 72.32 sec(s) Train Acc: 0.993071 Loss: 0.000328 | Val Acc: 0.950287 loss: 0.002805\n",
      "[065/080] 72.29 sec(s) Train Acc: 0.995075 Loss: 0.000212 | Val Acc: 0.949486 loss: 0.004756\n",
      "[066/080] 72.33 sec(s) Train Acc: 0.995304 Loss: 0.000232 | Val Acc: 0.952559 loss: 0.003242\n",
      "[067/080] 72.27 sec(s) Train Acc: 0.995476 Loss: 0.000185 | Val Acc: 0.955365 loss: 0.003304\n",
      "[068/080] 72.33 sec(s) Train Acc: 0.996106 Loss: 0.000179 | Val Acc: 0.946946 loss: 0.005347\n",
      "[069/080] 72.35 sec(s) Train Acc: 0.995362 Loss: 0.000204 | Val Acc: 0.952425 loss: 0.003761\n",
      "[070/080] 72.30 sec(s) Train Acc: 0.996049 Loss: 0.000183 | Val Acc: 0.951089 loss: 0.004644\n",
      "[071/080] 72.33 sec(s) Train Acc: 0.994904 Loss: 0.000208 | Val Acc: 0.952960 loss: 0.004096\n",
      "[072/080] 72.28 sec(s) Train Acc: 0.994789 Loss: 0.000218 | Val Acc: 0.946679 loss: 0.004695\n",
      "[073/080] 72.30 sec(s) Train Acc: 0.994674 Loss: 0.000226 | Val Acc: 0.950955 loss: 0.004943\n",
      "[074/080] 72.30 sec(s) Train Acc: 0.995362 Loss: 0.000201 | Val Acc: 0.942937 loss: 0.005323\n",
      "[075/080] 72.31 sec(s) Train Acc: 0.995820 Loss: 0.000187 | Val Acc: 0.951223 loss: 0.005024\n",
      "[076/080] 72.30 sec(s) Train Acc: 0.994904 Loss: 0.000255 | Val Acc: 0.954697 loss: 0.002824\n",
      "[077/080] 72.28 sec(s) Train Acc: 0.994732 Loss: 0.000239 | Val Acc: 0.952025 loss: 0.003733\n",
      "[078/080] 72.31 sec(s) Train Acc: 0.995648 Loss: 0.000195 | Val Acc: 0.950287 loss: 0.003740\n",
      "[079/080] 72.31 sec(s) Train Acc: 0.995877 Loss: 0.000192 | Val Acc: 0.951223 loss: 0.004903\n",
      "[080/080] 72.30 sec(s) Train Acc: 0.996507 Loss: 0.000189 | Val Acc: 0.954831 loss: 0.003675\n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "# model.train()\n",
    "epochs = 80\n",
    "for epoch in range(epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    train_acc =0.0\n",
    "    val_acc =0.0\n",
    "    train_loss = 0.0\n",
    "    val_loss =0.0\n",
    "    model.train()\n",
    "    for i,data in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        x,y = data[0].to(device),data[1].to(device)\n",
    "        # print(x.shape)\n",
    "        y_pred = model(x)\n",
    "        loss = cirection(y_pred,y.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_acc +=np.sum(np.argmax(y_pred.cpu().data.numpy(),axis=1)== y.cpu().numpy())\n",
    "        train_loss +=loss.item()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i,data in enumerate(test_loader):\n",
    "            valx ,valy = data[0].to(device),data[1].to(device)\n",
    "            val_pred = model(valx)\n",
    "            batch_loss = cirection(val_pred,valy.long())\n",
    "            val_acc +=np.sum(np.argmax(val_pred.cpu().data.numpy(),axis=1)== valy.cpu().numpy())\n",
    "            val_loss +=batch_loss.item()\n",
    "\n",
    "        print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f | Val Acc: %3.6f loss: %3.6f' % \\\n",
    "            (epoch + 1, epochs, time.time()-epoch_start_time, \\\n",
    "             train_acc/train_set.__len__(), train_loss/train_set.__len__(), val_acc/test_set.__len__(), val_loss/test_set.__len__()))\n",
    "\n",
    "torch.save(model,'cat_dog_128.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[080/080] 23308.28 sec(s) Train Acc: 0.996507 Loss: 0.000189 | Val Acc: 0.954831 loss: 0.003675\n"
     ]
    }
   ],
   "source": [
    "print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f | Val Acc: %3.6f loss: %3.6f' % \\\n",
    "            (epoch + 1, epochs, time.time()-epoch_start_time, \\\n",
    "             train_acc/train_set.__len__(), train_loss/train_set.__len__(), val_acc/test_set.__len__(), val_loss/test_set.__len__()))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0843aa2147bb7b68e1331c060614b1ebfeaba0f0db744f4b489daeb337a1f0b2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
