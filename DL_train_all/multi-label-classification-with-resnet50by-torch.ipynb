{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-14T15:06:26.117715Z","iopub.status.busy":"2021-12-14T15:06:26.117432Z","iopub.status.idle":"2021-12-14T15:06:28.217306Z","shell.execute_reply":"2021-12-14T15:06:28.21631Z","shell.execute_reply.started":"2021-12-14T15:06:26.117685Z"},"trusted":true},"outputs":[],"source":["import torch\n","import cv2\n","import numpy as np\n","import pandas as pd \n","import torchvision.transforms as transforms\n","from torch.utils.data import Dataset,DataLoader\n","import torch.nn as nn\n","from tqdm import tqdm\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","import matplotlib\n","matplotlib.style.use('ggplot')\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)"]},{"cell_type":"markdown","metadata":{},"source":["#### 电影多标签分类任务\n"]},{"cell_type":"markdown","metadata":{},"source":["make dataset to read "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-14T15:06:28.219283Z","iopub.status.busy":"2021-12-14T15:06:28.219031Z","iopub.status.idle":"2021-12-14T15:06:28.232766Z","shell.execute_reply":"2021-12-14T15:06:28.232181Z","shell.execute_reply.started":"2021-12-14T15:06:28.219252Z"},"trusted":true},"outputs":[],"source":["#dataset\n","class ImageDataset(Dataset):\n","    def __init__(self, csv, train, test):\n","        self.csv = csv\n","        self.train = train\n","        self.test = test\n","        self.all_image_names = self.csv[:]['Id']\n","        self.all_labels = np.array(self.csv.drop(['Id', 'Genre'], axis=1))\n","        self.train_ratio = int(0.85 * len(self.csv))\n","        self.valid_ratio = len(self.csv) - self.train_ratio\n","\n","        # set the training data images and labels\n","        if self.train == True:\n","            print(f\"Number of training images: {self.train_ratio}\")\n","            self.image_names = list(self.all_image_names[:self.train_ratio])\n","            self.labels = list(self.all_labels[:self.train_ratio])\n","\n","            # define the training transforms\n","            self.transform = transforms.Compose([\n","                transforms.ToPILImage(),\n","                transforms.Resize((400, 400)),\n","                transforms.RandomHorizontalFlip(p=0.5),\n","                transforms.RandomRotation(degrees=45),\n","                transforms.ToTensor(),\n","            ])\n","\n","        # set the validation data images and labels\n","        elif self.train == False and self.test == False:\n","            print(f\"Number of validation images: {self.valid_ratio}\")\n","            self.image_names = list(self.all_image_names[-self.valid_ratio:-10])\n","            self.labels = list(self.all_labels[-self.valid_ratio:])\n","\n","            # define the validation transforms\n","            self.transform = transforms.Compose([\n","                transforms.ToPILImage(),\n","                transforms.Resize((400, 400)),\n","                transforms.ToTensor(),\n","            ])\n","\n","        # set the test data images and labels, only last 10 images\n","        # this, we will use in a separate inference script\n","        elif self.test == True and self.train == False:\n","            self.image_names = list(self.all_image_names[-10:])\n","            self.labels = list(self.all_labels[-10:])\n","\n","             # define the test transforms\n","            self.transform = transforms.Compose([\n","                transforms.ToPILImage(),\n","                transforms.ToTensor(),\n","            ])\n","\n","    def __len__(self):\n","        return len(self.image_names)\n","    \n","    def __getitem__(self, index):\n","        image = cv2.imread(f\"../input/movie-classifier/Multi_Label_dataset/Images/{self.image_names[index]}.jpg\")\n","        # convert the image from BGR to RGB color format\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        # apply image transforms\n","        image = self.transform(image)\n","        targets = self.labels[index]\n","        \n","        return {\n","            'image': torch.tensor(image, dtype=torch.float32),\n","            'label': torch.tensor(targets, dtype=torch.float32)\n","        }\n"]},{"cell_type":"markdown","metadata":{},"source":["import model, here use the resnet50 and add a linear layer change 2048 to 25 "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-14T15:06:28.23431Z","iopub.status.busy":"2021-12-14T15:06:28.233827Z","iopub.status.idle":"2021-12-14T15:06:28.250895Z","shell.execute_reply":"2021-12-14T15:06:28.25016Z","shell.execute_reply.started":"2021-12-14T15:06:28.234249Z"},"trusted":true},"outputs":[],"source":["from torchvision import models as models\n","def model(pretrained, requires_grad):\n","    model = models.resnet50(progress=True, pretrained=pretrained)\n","    # to freeze the hidden layers\n","    if requires_grad == False:\n","        for param in model.parameters():\n","            param.requires_grad = False\n","    # to train the hidden layers\n","    elif requires_grad == True:\n","        for param in model.parameters():\n","            param.requires_grad = True\n","    # make the classification layer learnable\n","    # we have 25 classes in total\n","    model.fc = nn.Linear(2048, 25)\n","    return model"]},{"cell_type":"markdown","metadata":{},"source":["add some evaluation indicators, such as mAP,AUC and F1 score"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-14T15:06:28.252703Z","iopub.status.busy":"2021-12-14T15:06:28.252381Z","iopub.status.idle":"2021-12-14T15:06:29.111917Z","shell.execute_reply":"2021-12-14T15:06:29.111151Z","shell.execute_reply.started":"2021-12-14T15:06:28.252665Z"},"trusted":true},"outputs":[],"source":["from sklearn.metrics import average_precision_score,roc_auc_score\n","def compute_mAP(outputs,labels):\n","    y_true = labels.cpu().detach().numpy()\n","    y_pred = outputs.cpu().detach().numpy()\n","    AP = []\n","    for i in range(y_true.shape[0]):\n","        AP.append(average_precision_score(y_true[i],y_pred[i]))\n","    return np.mean(AP)\n","\n","def comput_roc(outputs,labels):\n","    y_true = labels.cpu().detach().numpy()\n","    y_pred = outputs.cpu().detach().numpy()\n","    ROC = []\n","    for i in range(y_true.shape[0]):\n","        ROC.append(roc_auc_score(y_true[i],y_pred[i]))\n","    return np.mean(ROC)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-14T15:06:29.114488Z","iopub.status.busy":"2021-12-14T15:06:29.114206Z","iopub.status.idle":"2021-12-14T15:06:29.125453Z","shell.execute_reply":"2021-12-14T15:06:29.12466Z","shell.execute_reply.started":"2021-12-14T15:06:29.114455Z"},"trusted":true},"outputs":[],"source":["from sklearn.metrics import precision_score,recall_score,f1_score\n","def calculate_metrics(pred, target, threshold=0.5):\n","    pred = np.array(pred > threshold, dtype=float)\n","    return {'micro/precision': precision_score(y_true=target, y_pred=pred, average='micro'),\n","            'micro/recall': recall_score(y_true=target, y_pred=pred, average='micro'),\n","            'micro/f1': f1_score(y_true=target, y_pred=pred, average='micro'),\n","            'macro/precision': precision_score(y_true=target, y_pred=pred, average='macro'),\n","            'macro/recall': recall_score(y_true=target, y_pred=pred, average='macro'),\n","            'macro/f1': f1_score(y_true=target, y_pred=pred, average='macro'),\n","            'samples/precision': precision_score(y_true=target, y_pred=pred, average='samples'),\n","            'samples/recall': recall_score(y_true=target, y_pred=pred, average='samples'),\n","            'samples/f1': f1_score(y_true=target, y_pred=pred, average='samples'),\n","            }"]},{"cell_type":"markdown","metadata":{},"source":["train function and valid function"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-14T15:06:29.128664Z","iopub.status.busy":"2021-12-14T15:06:29.127949Z","iopub.status.idle":"2021-12-14T15:06:29.137967Z","shell.execute_reply":"2021-12-14T15:06:29.137248Z","shell.execute_reply.started":"2021-12-14T15:06:29.128616Z"},"trusted":true},"outputs":[],"source":["# training function\n","def train(model, dataloader, optimizer, criterion, train_data, device):\n","    print('Training')\n","    model.train()\n","    counter = 0\n","    train_running_loss = 0.0\n","    train_running_mAP = 0.0\n","    train_running_ROC = 0.0\n","    for i, data in tqdm(enumerate(dataloader), total=int(len(train_data)/dataloader.batch_size)):\n","        counter += 1\n","        data, target = data['image'].to(device), data['label'].to(device)\n","        optimizer.zero_grad()\n","        outputs = model(data)\n","        # apply sigmoid activation to get all the outputs between 0 and 1\n","        outputs = torch.sigmoid(outputs)\n","        loss = criterion(outputs, target)\n","        train_running_loss += loss.item()\n","        train_running_mAP += compute_mAP(outputs,target)\n","        train_running_ROC += comput_roc(outputs,target)\n","        # backpropagation\n","        loss.backward()\n","        # update optimizer parameters\n","        optimizer.step()\n","        \n","    train_loss = train_running_loss / counter\n","    train_mAP = train_running_mAP/counter\n","    train_ROC = train_running_ROC/counter\n","    print(f\"Train mAP: {train_mAP:.4f}\")\n","    print(f'Train ROC: {train_ROC:.4f}')\n","    return train_loss,train_mAP,train_ROC"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-14T15:06:29.139817Z","iopub.status.busy":"2021-12-14T15:06:29.139291Z","iopub.status.idle":"2021-12-14T15:06:29.151649Z","shell.execute_reply":"2021-12-14T15:06:29.150994Z","shell.execute_reply.started":"2021-12-14T15:06:29.139776Z"},"trusted":true},"outputs":[],"source":["# validation function\n","def validate(model, dataloader, criterion, val_data, device):\n","    print('Validating')\n","    model.eval()\n","    counter = 0\n","    val_running_loss = 0.0\n","    val_running_mAP = 0.0\n","    val_running_ROC = 0.0\n","    with torch.no_grad():\n","        model_result = []\n","        targets = []\n","        for i, data in tqdm(enumerate(dataloader), total=int(len(val_data)/dataloader.batch_size)):\n","            counter += 1\n","            data, target = data['image'].to(device), data['label'].to(device)\n","            outputs = model(data)\n","            # apply sigmoid activation to get all the outputs between 0 and 1\n","            outputs = torch.sigmoid(outputs)\n","            loss = criterion(outputs, target)\n","            model_result.extend(outputs.cpu().numpy())\n","            targets.extend(target.cpu().numpy())\n","            val_running_mAP += compute_mAP(outputs,target)\n","            val_running_ROC += comput_roc(outputs,target)\n","            val_running_loss += loss.item()\n","            \n","        result = calculate_metrics(np.array(model_result), np.array(targets))\n","        val_loss = val_running_loss / counter\n","        val_mAP = val_running_mAP/counter\n","        val_ROC = val_running_ROC/counter\n","        print(\"epoch:{:2d}  test: \"\n","                  \"micro f1: {:.3f} \"\n","                  \"macro f1: {:.3f} \"\n","                  \"samples f1: {:.3f}\".format(epoch,\n","                                              result['micro/f1'],\n","                                              result['macro/f1'],\n","                                              result['samples/f1']))\n","        print(f\"Val mAP: {val_mAP:.4f}\")\n","        print(f'Val ROC: {val_ROC:.4f}')\n","        return val_loss,val_mAP,val_ROC"]},{"cell_type":"markdown","metadata":{},"source":["some settings and you can train and output some evaluations "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-14T15:07:16.220638Z","iopub.status.busy":"2021-12-14T15:07:16.219994Z","iopub.status.idle":"2021-12-14T15:28:17.015718Z","shell.execute_reply":"2021-12-14T15:28:17.015021Z","shell.execute_reply.started":"2021-12-14T15:07:16.220602Z"},"trusted":true},"outputs":[],"source":["#train\n","# initialize the computation device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model2 = model(pretrained=True, requires_grad=False).to(device)\n","# learning parameters\n","lr = 0.0001\n","epochs = 10\n","batch_size = 32\n","optimizer = optim.Adam(model2.parameters(), lr=lr)\n","criterion = nn.BCELoss()\n","# read the training csv file\n","train_csv = pd.read_csv('../input/movie-classifier/Multi_Label_dataset/train.csv')\n","# train dataset\n","train_data = ImageDataset(\n","    train_csv, train=True, test=False\n",")\n","# validation dataset\n","valid_data = ImageDataset(\n","    train_csv, train=False, test=False\n",")\n","# train data loader\n","train_loader = DataLoader(\n","    train_data, \n","    batch_size=batch_size,\n","    shuffle=True\n",")\n","# validation data loader\n","valid_loader = DataLoader(\n","    valid_data, \n","    batch_size=batch_size,\n","    shuffle=False\n",")\n","train_loss = []\n","valid_loss = []\n","train_mAP = []\n","train_ROC = []\n","valid_mAP = []\n","valid_ROC = []\n","for epoch in range(epochs):\n","    print(f\"Epoch {epoch+1} of {epochs}\")\n","    train_epoch_loss,train_epoch_mAP,train_epoch_ROC = train(\n","        model2, train_loader, optimizer, criterion, train_data, device\n","    )\n","    valid_epoch_loss,val_epoch_mAP,val_epoch_ROC = validate(\n","        model2, valid_loader, criterion, valid_data, device\n","    )\n","    train_loss.append(train_epoch_loss)\n","    valid_loss.append(valid_epoch_loss)\n","    train_mAP.append(train_epoch_mAP)\n","    train_ROC.append(train_epoch_ROC)\n","    valid_mAP.append(val_epoch_mAP)\n","    valid_ROC.append(val_epoch_ROC)\n","    print(f\"Train Loss: {train_epoch_loss:.4f}\")\n","    print(f'Val Loss: {valid_epoch_loss:.4f}')"]},{"cell_type":"markdown","metadata":{},"source":["save the model and have a look at the loss"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-14T15:29:35.182038Z","iopub.status.busy":"2021-12-14T15:29:35.181729Z","iopub.status.idle":"2021-12-14T15:29:35.787682Z","shell.execute_reply":"2021-12-14T15:29:35.786758Z","shell.execute_reply.started":"2021-12-14T15:29:35.182005Z"},"trusted":true},"outputs":[],"source":["torch.save({\n","            'epoch': epochs,\n","            'model_state_dict': model2.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'loss': criterion,\n","            }, './model.pth')\n","# plot and save the train and validation line graphs\n","plt.figure(figsize=(10, 7))\n","plt.plot(train_loss, color='orange', label='train loss')\n","plt.plot(valid_loss, color='red', label='validataion loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.savefig('../loss.png')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["watch the mAP and ROC"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-14T15:29:54.013552Z","iopub.status.busy":"2021-12-14T15:29:54.013242Z","iopub.status.idle":"2021-12-14T15:29:54.360552Z","shell.execute_reply":"2021-12-14T15:29:54.359824Z","shell.execute_reply.started":"2021-12-14T15:29:54.013523Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize=(10, 7))\n","plt.plot(train_mAP, color='orange', label='train_mAP')\n","plt.plot(train_ROC, color='red', label='train_ROC')\n","plt.plot(valid_mAP, color='blue',label ='valid_mAP')\n","plt.plot(valid_ROC, color='pink', label='valid_ROC')\n","plt.xlabel('Epochs')\n","plt.ylabel('ACC')\n","plt.legend()\n","plt.savefig('../ACC.png')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["to do some perdict and visulation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-14T15:30:31.083728Z","iopub.status.busy":"2021-12-14T15:30:31.083443Z","iopub.status.idle":"2021-12-14T15:30:31.603382Z","shell.execute_reply":"2021-12-14T15:30:31.602691Z","shell.execute_reply.started":"2021-12-14T15:30:31.083698Z"},"trusted":true},"outputs":[],"source":["#watch the predict \n","# initialize the computation device\n","net = model(pretrained=False, requires_grad=False).to(device)\n","# load the model checkpoint\n","checkpoint = torch.load('./model.pth')\n","# load model weights state_dict\n","net.load_state_dict(checkpoint['model_state_dict'])\n","net.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-14T15:30:47.73965Z","iopub.status.busy":"2021-12-14T15:30:47.739392Z","iopub.status.idle":"2021-12-14T15:30:49.130633Z","shell.execute_reply":"2021-12-14T15:30:49.129975Z","shell.execute_reply.started":"2021-12-14T15:30:47.739621Z"},"trusted":true},"outputs":[],"source":["train_csv = pd.read_csv('../input/movie-classifier/Multi_Label_dataset/train.csv')\n","genres = train_csv.columns.values[2:]\n","# prepare the test dataset and dataloader\n","test_data = ImageDataset(\n","    train_csv, train=False, test=True\n",")\n","test_loader = DataLoader(\n","    test_data, \n","    batch_size=1,\n","    shuffle=False\n",")\n","for counter, data in enumerate(test_loader):\n","    image, target = data['image'].to(device), data['label']\n","    # get all the index positions where value == 1\n","    target_indices = [i for i in range(len(target[0])) if target[0][i] == 1]\n","    # get the predictions by passing the image through the model\n","    outputs = net(image)\n","    outputs = torch.sigmoid(outputs)\n","    outputs = outputs.detach().cpu()\n","    sorted_indices = np.argsort(outputs[0])\n","    best = sorted_indices[-3:]\n","    string_predicted = ''\n","    string_actual = ''\n","    for i in range(len(best)):\n","        string_predicted += f\"{genres[best[i]]}    \"\n","    for i in range(len(target_indices)):\n","        string_actual += f\"{genres[target_indices[i]]}    \"\n","    image = image.squeeze(0)\n","    image = image.detach().cpu().numpy()\n","    image = np.transpose(image, (1, 2, 0))\n","    plt.imshow(image)\n","    plt.axis('off')\n","    plt.title(f\"PREDICTED: {string_predicted}\\nACTUAL: {string_actual}\")\n","    plt.savefig(f\"./inference_{counter}.jpg\")\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
