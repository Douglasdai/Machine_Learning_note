{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "#关于自然语言处理 的RNN 以后可以用GRU 和lstm\r\n",
    "\"\"\"\r\n",
    "本次作業是要讓同學接觸 NLP 當中一個簡單的 task —— 語句分類（文本分類）\r\n",
    "\r\n",
    "給定一個語句，判斷他有沒有惡意（負面標 1，正面標 0）\r\n",
    "\"\"\"\r\n",
    "import numpy as np\r\n",
    "import torch \r\n",
    "import cv2\r\n",
    "import torch.nn as nn \r\n",
    "import pandas as pd\r\n",
    "import os\r\n",
    "from torch.utils.data import DataLoader,Dataset\r\n",
    "import torchvision.transforms as transforms\r\n",
    "import time\r\n",
    "import torch.nn.functional as F"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import warnings\r\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "#Uitiles\r\n",
    "#定义常用函数\r\n",
    "def load_traing_data(path='./data/training_label.txt'):\r\n",
    "    if 'training_label' in path:\r\n",
    "        with open (path,'r',encoding='UTF-8') as f:\r\n",
    "            lines = f.readlines()\r\n",
    "            lines = [line.strip('\\n').split(' ') for line in lines]\r\n",
    "        x = [line[2:]for line in lines]\r\n",
    "        y = [line[0] for line in lines]\r\n",
    "        return x,y\r\n",
    "    else:\r\n",
    "        with open(path,'r',encoding='UTF-8') as f:\r\n",
    "            lines = f.readlines()\r\n",
    "            x = [line.strip('\\n').split(' ')for line in lines]\r\n",
    "            return x\r\n",
    "\r\n",
    "def load_testing_data(path='./data/testing_data.txt'):\r\n",
    "    with open (path,'r',encoding='UTF-8') as f:\r\n",
    "        lines = f.readlines()\r\n",
    "        X = [\"\".join(line.strip('\\n').split(',')[1:]).strip() for line in lines[1:]]\r\n",
    "        X = [sen.split(' ')for sen in X]\r\n",
    "        return X\r\n",
    "\r\n",
    "def evaluation(outputs,lables):\r\n",
    "    outputs[outputs>=0.5] = 1\r\n",
    "    outputs[outputs<0.5] = 0\r\n",
    "    correct = torch.sum(torch.eq(outputs,lables)).item()\r\n",
    "    return correct"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "#Word2vec\r\n",
    "import os\r\n",
    "import argparse\r\n",
    "from gensim.models import word2vec\r\n",
    "def train_word2vec(x):\r\n",
    "    model = word2vec.Word2Vec(x,size=250,window=5, min_count=5, workers=12, iter=10, sg=1)\r\n",
    "    return model\r\n",
    "\r\n",
    "print(\"loading training data...\")\r\n",
    "train_x,y = load_traing_data(\"./data/training_label.txt\")\r\n",
    "train_x_nolable =load_traing_data('./data/training_nolabel.txt')\r\n",
    "\r\n",
    "print(\"loading testing data\")\r\n",
    "test_x = load_testing_data('./data/testing_data.txt')\r\n",
    "flag =False\r\n",
    "if flag == False:\r\n",
    "    model = train_word2vec(train_x+test_x)\r\n",
    "    print('saving model...')\r\n",
    "    model.save('w2v_all.model')\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loading training data...\n",
      "loading testing data\n",
      "saving model...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "#data process\r\n",
    "#数据预处理\r\n",
    "import torch.nn as nn\r\n",
    "from gensim.models import Word2Vec\r\n",
    "\r\n",
    "class PreProcess():\r\n",
    "    def __init__(self,sentences,sen_len,w2v_path='./w2v_all.model'):\r\n",
    "        self.w2v_path = w2v_path\r\n",
    "        self.sentences = sentences\r\n",
    "        self.sen_len = sen_len\r\n",
    "        self.idx2word = []\r\n",
    "        self.word2idx = {}\r\n",
    "        self.embedding_matix = []\r\n",
    "    \r\n",
    "    def get_w2v_model(self):\r\n",
    "        self.embedding = Word2Vec.load(self.w2v_path)\r\n",
    "        self.embedding_dim = self.embedding.vector_size\r\n",
    "    \r\n",
    "    def add_embedding(self,word):\r\n",
    "        vector = torch.empty(1,self.embedding_dim)\r\n",
    "        nn.init.uniform(vector)\r\n",
    "        self.word2idx[word] =len(self.word2idx)\r\n",
    "        self.idx2word.append(word)\r\n",
    "        self.embedding_matix = torch.cat([self.embedding_matix,vector],0)\r\n",
    "    \r\n",
    "    def make_embedding(self,load =True):\r\n",
    "        print('Get embedding...')\r\n",
    "        if load:\r\n",
    "            print('loading word to vec model...')\r\n",
    "            self.get_w2v_model()\r\n",
    "        else:\r\n",
    "            raise NotImplementedError\r\n",
    "        \r\n",
    "        for i,word in enumerate(self.embedding.wv.vocab):\r\n",
    "            print('get words #{}'.format(i+1),end='\\r')\r\n",
    "            self.word2idx[word] = len(self.word2idx)\r\n",
    "            self.idx2word.append(word)\r\n",
    "            self.embedding_matix.append(self.embedding[word])\r\n",
    "        print('')\r\n",
    "        self.embedding_matix = torch.tensor(self.embedding_matix)\r\n",
    "        self.add_embedding(\"<PAD>\")\r\n",
    "        self.add_embedding(\"<UNK>\")\r\n",
    "        print(\"total words: {}\".format(len(self.embedding_matix)))\r\n",
    "        return self.embedding_matix\r\n",
    "\r\n",
    "    def pad_sequence(self,sentence):\r\n",
    "        if len(sentence)> self.sen_len:\r\n",
    "            sentence = sentence[:self.sen_len]\r\n",
    "        else:\r\n",
    "            pad_len = self.sen_len -len(sentence)\r\n",
    "            for _ in range(pad_len):\r\n",
    "                sentence.append(self.word2idx[\"<PAD>\"])\r\n",
    "        assert len(sentence) == self.sen_len\r\n",
    "        return sentence\r\n",
    "\r\n",
    "    def sentence_word2idx(self):\r\n",
    "        sentence_list =[]\r\n",
    "        for i,sen in enumerate(self.sentences):\r\n",
    "            print('sentence count #{}'.format(i+1),end ='\\r')\r\n",
    "            sentence_idx = []\r\n",
    "            for word in sen:\r\n",
    "                if(word in self.word2idx.keys()):\r\n",
    "                    sentence_idx.append(self.word2idx[word])\r\n",
    "                else:\r\n",
    "                    sentence_idx.append(self.word2idx[\"<UNK>\"])\r\n",
    "            #每个句子变成一样的长度\r\n",
    "            sentence_idx = self.pad_sequence(sentence_idx)\r\n",
    "            sentence_list.append(sentence_idx)\r\n",
    "        return torch.LongTensor(sentence_list)\r\n",
    "    \r\n",
    "    def labels_to_tensor(self,y):\r\n",
    "        y = [int(label) for label in y]\r\n",
    "        return torch.LongTensor(y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "#dataset\r\n",
    "from torch.utils import data\r\n",
    "class TwitterDataset(data.Dataset):\r\n",
    "    def __init__(self,X,Y):\r\n",
    "        self.data = X\r\n",
    "        self.label = Y \r\n",
    "    \r\n",
    "    def __getitem__(self, index):\r\n",
    "        if self.label is None:return self.data[index]\r\n",
    "        return self.data[index],self.label[index]\r\n",
    "    \r\n",
    "    def __len__(self):\r\n",
    "        return len(self.data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "#model\r\n",
    "class RNN(nn.Module):\r\n",
    "    def __init__(self,embedding,embedding_dim,hidden_dim,num_layer,dropout=0.5,fix_embedding=True):\r\n",
    "        super(RNN,self).__init__()\r\n",
    "        #embedding layer\r\n",
    "        self.embedding = nn.Embedding(embedding.size(0),embedding.size(1))\r\n",
    "        self.embedding.weight = nn.Parameter(embedding)\r\n",
    "        #是否embedding fix 住\r\n",
    "        self.embedding.weight.requires_grad = False if fix_embedding else True\r\n",
    "        self.embedding_dim = embedding_dim\r\n",
    "        self.hidden_dim = hidden_dim\r\n",
    "        self.num_layers = num_layer\r\n",
    "        self.dropout = dropout\r\n",
    "        self.lstm = nn.LSTM(embedding_dim,hidden_dim,num_layers=num_layer,batch_first = True)\r\n",
    "        self.classfier = nn.Sequential(\r\n",
    "            nn.Dropout(dropout),\r\n",
    "            nn.Linear(hidden_dim,1),\r\n",
    "            nn.ReLU()\r\n",
    "        )\r\n",
    "\r\n",
    "    def forward(self,X):\r\n",
    "        X = self.embedding(X)\r\n",
    "        out,_ = self.lstm(X,None)\r\n",
    "        out = out[:,-1,:]\r\n",
    "        out = self.classfier(out)\r\n",
    "        return out\r\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "#setting\r\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
    "sen_len =20\r\n",
    "fix_embedding = True\r\n",
    "batch_size = 16\r\n",
    "epoch =5\r\n",
    "lr =0.001\r\n",
    "print(device)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "#inputs and label preprocess\r\n",
    "preprocess = PreProcess(train_x,sen_len)\r\n",
    "embedding = preprocess.make_embedding(load=True)\r\n",
    "train_x = preprocess.sentence_word2idx()\r\n",
    "y = preprocess.labels_to_tensor(y)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Get embedding...\n",
      "loading word to vec model...\n",
      "get words #24694\n",
      "total words: 24696\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "#build model\r\n",
    "model = RNN(embedding,embedding_dim=250,hidden_dim=150,num_layer=1,dropout=0.5,fix_embedding=fix_embedding)\r\n",
    "model = model.to(device)\r\n",
    "# model.eval()\r\n",
    "\"\"\"\r\n",
    "RNN(\r\n",
    "  (embedding): Embedding(24696, 250)\r\n",
    "  (lstm): LSTM(250, 150, batch_first=True)\r\n",
    "  (classfier): Sequential(\r\n",
    "    (0): Dropout(p=0.5, inplace=False)\r\n",
    "    (1): Linear(in_features=150, out_features=1, bias=True)\r\n",
    "    (2): ReLU()\r\n",
    "  )\r\n",
    ")\r\n",
    "\"\"\""
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(24696, 250)\n",
       "  (lstm): LSTM(250, 150, batch_first=True)\n",
       "  (classfier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=150, out_features=1, bias=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "X_train,X_val ,y_train,y_val = train_x[:18000],train_x[18000:],y[:18000],y[18000:]\r\n",
    "train_dataset = TwitterDataset(X = X_train,Y =y_train)\r\n",
    "val_dataset = TwitterDataset(X= X_val,Y = y_val)\r\n",
    "train_loader  = DataLoader(dataset= train_dataset,batch_size=batch_size,shuffle=True)\r\n",
    "val_loader = DataLoader(dataset= val_dataset,batch_size = batch_size,shuffle=False)\r\n",
    "# for i,(inputs,labels) in enumerate(train_loader):\r\n",
    "#     print(inputs[15])\r\n",
    "#     break\r\n",
    "    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([ 1842, 24695,    66,    68,  1972,  1973,  1974, 24694, 24694, 24694,\n",
      "        24694, 24694, 24694, 24694, 24694, 24694, 24694, 24694, 24694, 24694])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "#training\r\n",
    "import torch.optim as optim\r\n",
    "total = sum(p.numel() for p in model.parameters())\r\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n",
    "print('Start traing.. parameter total:{}, trainable:{}\\n'.format(total,trainable))\r\n",
    "model.train()\r\n",
    "criteion = nn.BCELoss()\r\n",
    "t_batch = len(train_loader)\r\n",
    "v_batch = len(val_loader)\r\n",
    "optimizer = optim.Adam(model.parameters(),lr = lr)\r\n",
    "best_acc = 0,0,0\r\n",
    "for e in range(epoch):\r\n",
    "    total_loss,total_acc =0.0,0.0\r\n",
    "    for i,(inputs,labels) in enumerate(train_loader):\r\n",
    "        inputs = inputs.to(device)\r\n",
    "        labels = labels.to(device)\r\n",
    "        optimizer.zero_grad()\r\n",
    "        outputs = model(inputs)\r\n",
    "        outputs = outputs.squeeze()# 去掉最外面的 dimension，好讓 outputs 可以餵進 criterion()\r\n",
    "        loss = criteion(outputs,labels)\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "        correct = evaluation(outputs,labels)\r\n",
    "        total_acc +=(correct/batch_size)\r\n",
    "        total_loss +=loss.item()\r\n",
    "        print('[ Epoch{}: {}/{} ] loss:{:.3f} acc:{:.3f} '.format(\r\n",
    "            \tepoch+1, i+1, t_batch, loss.item(), correct*100/batch_size), end='\\r')\r\n",
    "    print('\\nTrain | Loss:{:.5f} Acc: {:.3f}'.format(total_loss/t_batch, total_acc/t_batch*100))\r\n",
    "\r\n",
    "    model.eval()\r\n",
    "    with torch.no_grad():\r\n",
    "        val_loss ,val_acc = 0.0,0.0\r\n",
    "        for i,(inputs,labels) in enumerate(val_loader):\r\n",
    "            #val 验证\r\n",
    "            inputs = inputs.to(device,dtype = torch.long)\r\n",
    "            labels = labels.to(device,dtype = torch.float)\r\n",
    "            outputs = model(inputs)\r\n",
    "            outputs = outputs.squeeze()\r\n",
    "            loss = criteion(outputs,labels)\r\n",
    "            correct = evaluation(outputs,labels)\r\n",
    "            total_acc +=(correct/batch_size)\r\n",
    "            total_loss +=loss.item()\r\n",
    "\r\n",
    "            print(\"Valid | Loss:{:.5f} Acc: {:.3f} \".format(total_loss/v_batch, total_acc/v_batch*100))\r\n",
    "            if total_acc >best_acc:\r\n",
    "                best_acc = total_acc\r\n",
    "                torch.save(model,'ckpt-{}.model'.format(total_acc/v_batch*100))\r\n",
    "    model.train()       \r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Start traing.. parameter total:6415351, trainable:241351\n",
      "\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-38d297bf0993>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mtotal_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtotal_acc\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "#testing data input\r\n",
    "print(\"loading testing data ...\")\r\n",
    "test_x = load_testing_data()\r\n",
    "preprocess = PreProcess(test_x, sen_len)\r\n",
    "embedding = preprocess.make_embedding(load=True)\r\n",
    "test_x = preprocess.sentence_word2idx()\r\n",
    "test_dataset = TwitterDataset(X=test_x, Y=None)\r\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\r\n",
    "                                            batch_size = batch_size,\r\n",
    "                                            shuffle = False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loading testing data ...\n",
      "Get embedding...\n",
      "loading word to vec model...\n",
      "get words #24694\n",
      "total words: 24696\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Testing \r\n",
    "\r\n",
    "model.eval()\r\n",
    "ret_output = []\r\n",
    "with torch.no_grad():\r\n",
    "    for i,inputs in enumerate(test_loader):\r\n",
    "        inputs = inputs.to(device,dtype = torch.long)\r\n",
    "        outputs = model(inputs)\r\n",
    "        outputs = outputs.squeeze()\r\n",
    "        outputs[outputs>=0.5] = 1 # 大於等於 0.5 為正面\r\n",
    "        outputs[outputs<0.5] = 0 # 小於 0.5 為負面\r\n",
    "        ret_output += outputs.int().tolist()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#saving csv\r\n",
    "tmp = pd.DataFrame({\"id\":[str(i) for i in range(len(test_x))],\"label\":outputs})\r\n",
    "print(\"save csv ...\")\r\n",
    "tmp.to_csv(os.path.join(path_prefix, 'predict.csv'), index=False)\r\n",
    "print(\"Finish Predicting\")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.3",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit"
  },
  "interpreter": {
   "hash": "0843aa2147bb7b68e1331c060614b1ebfeaba0f0db744f4b489daeb337a1f0b2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}